2025-01-17 15:44:20 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:44:20 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:44:25 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 15:44:25 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:44:25 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:44:25 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 15:44:47 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 15:44:47 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 15:44:48 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/5v4Sozv6y96WANtN5Ia1
2025-01-17 15:44:48 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/5v4Sozv6y96WANtN5Ia1
2025-01-17 15:44:48 | INFO     | app.bot.interview_bot:__init__:42 - Initializing Interview Bot
2025-01-17 15:44:48 | DEBUG    | app.bot.interview_bot:__init__:43 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 15:44:48 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x11ce6e270>
2025-01-17 15:44:48 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/5v4Sozv6y96WANtN5Ia1 and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiNXY0U296djZ5OTZXQU50TjVJYTEiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTAyODgsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDY2ODh9.Lj7CTGLOa3L31LCvJ_NnHpLYWODVaBzuH3I8RgNotMg
2025-01-17 15:44:48 | INFO     | app.bot.interview_bot:start:168 - Starting interview session in room: https://getroborecruiter.daily.co/5v4Sozv6y96WANtN5Ia1
2025-01-17 15:44:48 | DEBUG    | app.bot.interview_bot:_init_daily_transport:59 - Initializing Daily transport
2025-01-17 15:44:48 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 15:44:48 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 15:44:48 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 15:44:48 | DEBUG    | app.bot.interview_bot:_init_tts_service:98 - Initializing Cartesia TTS
2025-01-17 15:44:48 | INFO     | app.bot.interview_bot:start:173 - Setting up interview pipeline
2025-01-17 15:44:48 | ERROR    | app.bot.interview_bot:start:179 - Error during interview session: LangchainProcessor.__init__() got an unexpected keyword argument 'api_key'
2025-01-17 15:44:48 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: LangchainProcessor.__init__() got an unexpected keyword argument 'api_key'
2025-01-17 15:44:48 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: LangchainProcessor.__init__() got an unexpected keyword argument 'api_key'
2025-01-17 15:46:59 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 15:47:00 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:47:00 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:47:01 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 15:47:01 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:47:01 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:47:01 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 15:47:24 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 15:47:25 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:47:25 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:47:26 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 15:47:26 | DEBUG    | app.core.config:validate_required_settings:30 - Validating required settings
2025-01-17 15:47:26 | INFO     | app.core.config:validate_required_settings:44 - All required settings validated successfully
2025-01-17 15:47:26 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 15:47:31 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 15:47:31 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 15:47:31 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w
2025-01-17 15:47:31 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w
2025-01-17 15:47:31 | INFO     | app.bot.interview_bot:__init__:42 - Initializing Interview Bot
2025-01-17 15:47:31 | DEBUG    | app.bot.interview_bot:__init__:43 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 15:47:31 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x129d72270>
2025-01-17 15:47:31 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoidWQwYkppMkZNamZtM1M4QUpMNnciLCJvIjp0cnVlLCJleHAiOjE3MzcxNTA0NTEsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDY4NTF9.tMzEseV4SD3_whUd02v00a7LS9C4V5O_Zy-ng0A3Q7U
2025-01-17 15:47:31 | INFO     | app.bot.interview_bot:start:188 - Starting interview session in room: https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w
2025-01-17 15:47:31 | DEBUG    | app.bot.interview_bot:_init_daily_transport:59 - Initializing Daily transport
2025-01-17 15:47:31 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 15:47:31 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 15:47:32 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 15:47:32 | DEBUG    | app.bot.interview_bot:_init_tts_service:98 - Initializing Cartesia TTS
2025-01-17 15:47:32 | INFO     | app.bot.interview_bot:start:193 - Setting up interview pipeline
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> LLMUserResponseAggregator#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 15:47:32 | DEBUG    | app.bot.interview_bot:_run_pipeline:166 - Starting pipeline runner
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 15:47:32 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 15:47:32 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 15:47:32 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 15:47:32 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w
2025-01-17 15:47:33 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/ud0bJi2FMjfm3S8AJL6w
2025-01-17 15:47:33 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 15:47:34 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'model': 'nova-2-general', 'transcriptId': '0cb11c3b-690b-4e24-8164-e7d7a0f35215', 'language': 'en', 'startedBy': 'ba2c39fa-1121-4229-9afe-16f4446c9a68'}
2025-01-17 15:47:44 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined cdd8d172-40e4-4145-b5b1-0b8de4edb8d3
2025-01-17 15:47:47 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 15:47:48 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 15:47:52 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 15:47:53 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 15:52:32 | WARNING  | pipecat.services.cartesia:_reconnect_websocket:279 - CartesiaTTSService#0 reconnecting (attempt: 1)
2025-01-17 15:52:32 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 15:52:32 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 15:53:21 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 15:53:21 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 15:53:21 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 15:53:21 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:04:31 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:04:31 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:04:33 | ERROR    | pipecat.services.deepgram:<module>:40 - Exception: No module named 'deepgram'
2025-01-17 16:04:33 | ERROR    | pipecat.services.deepgram:<module>:41 - In order to use Deepgram, you need to `pip install pipecat-ai[deepgram]`. Also, set `DEEPGRAM_API_KEY` environment variable.
2025-01-17 16:05:34 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:05:34 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:05:36 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:05:36 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:05:36 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:05:36 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:05:42 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:05:42 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:05:43 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:05:43 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:05:43 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:05:43 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:05:43 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10a8742f0>
2025-01-17 16:05:43 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoibGtMY2Rxc2lRTWhTZERUNkd0anEiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE1NDIsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDc5NDJ9.mOQzhEhiZImKH25STDbA4TCW-bFbniLGnihqweb7RFE
2025-01-17 16:05:43 | INFO     | app.bot.interview_bot:start:221 - Starting interview session in room: https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:05:43 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:05:43 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:05:43 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:05:43 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:05:43 | DEBUG    | app.bot.interview_bot:_init_tts_service:99 - Initializing Cartesia TTS
2025-01-17 16:05:43 | INFO     | app.bot.interview_bot:start:226 - Setting up interview pipeline
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 16:05:43 | DEBUG    | app.bot.interview_bot:_run_pipeline:199 - Starting pipeline runner
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 16:05:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 16:05:43 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 16:05:43 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 16:05:43 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:05:45 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:05:46 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:05:46 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 16:05:46 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'model': 'nova-2-general', 'transcriptId': '57f95d82-5bc2-43de-aa6e-f0e03407f952', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'language': 'en', 'startedBy': 'd32575f1-8834-4be2-aff0-371143858405'}
2025-01-17 16:05:54 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 77f753a0-7a7f-42d7-9a06-fc6630a6ceb0
2025-01-17 16:06:09 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:06:12 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:06:12 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 16:06:12 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Enabling transcription. Good. Good. Good.
2025-01-17 16:06:12 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x101427560>
                                      └ <function spawn_main at 0x101427560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x101427600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1011a8680>
           └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1011a3ba0>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1011b30e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    └ <function subprocess_started at 0x102141a80>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1020e2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x102140860>
           │       │   └ <uvicorn.server.Server object at 0x1020e2e40>
           │       └ <function run at 0x1014858a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1021617e0>
           │      └ <function Runner.run at 0x101ec5260>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101ec2b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101ec2ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101ec4900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101e39620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=947, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'user', 'content': 'Enabling transcription. ...
          │    └ <function LangchainProcessor.process_frame at 0x10a39e480>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10ce397f0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Enabling transcription. Good. Good. Good.'
          │    └ <function LangchainProcessor._ainvoke at 0x10a3dec00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10ce397f0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10a33ad40>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10ce397f0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10cf99ee0>
                       │    └ <function RunnableSequence.atransform at 0x10a33aca0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10a3394e0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x101ead1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10cff9360>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10cff9140>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d00cb00>, 'rec...
                        │    │       └ {'input': 'Enabling transcription. Good. Good. Good.'}
                        │    └ <function Runnable.astream at 0x10a3387c0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d00cb00>, 'rec...
                │    │       └ {'input': 'Enabling transcription. Good. Good. Good.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10abfb7e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10a339260>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10cfcd240>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10d0602e0>
                           │       └ <function create_task at 0x101ead1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Enabling transcription. Good. Good. Good.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10abfae80>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 16:07:35 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 16:07:35 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 16:07:35 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 16:07:35 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 16:07:35 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:07:35 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:07:35 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 16:07:35 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:07:35 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 16:07:35 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/lkLcdqsiQMhSdDT6Gtjq
2025-01-17 16:08:02 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:08:02 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:08:02 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG
2025-01-17 16:08:02 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG
2025-01-17 16:08:02 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:08:02 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:08:02 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10cf2a990>
2025-01-17 16:08:02 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiaFVZeXd3Zmo0bnlaekpXM0MzVkciLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE2ODIsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgwODJ9.d0ZkPM-YP5BoAp3KvNae8Z8EijLq3yrXz4xhmtEwT_M
2025-01-17 16:08:02 | INFO     | app.bot.interview_bot:start:221 - Starting interview session in room: https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG
2025-01-17 16:08:02 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:08:02 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:08:02 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:08:02 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:08:02 | DEBUG    | app.bot.interview_bot:_init_tts_service:99 - Initializing Cartesia TTS
2025-01-17 16:08:02 | INFO     | app.bot.interview_bot:start:226 - Setting up interview pipeline
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#1 -> DailyInputTransport#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#1 -> DeepgramSTTService#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#1 -> LLMUserResponseAggregator#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#1 -> LangchainProcessor#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#1 -> CartesiaTTSService#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#1 -> DailyOutputTransport#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#1 -> LLMAssistantResponseAggregator#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#1 -> PipelineSink#1
2025-01-17 16:08:03 | DEBUG    | app.bot.interview_bot:_run_pipeline:199 - Starting pipeline runner
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#1 -> Pipeline#1
2025-01-17 16:08:03 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#1 -> Sink#1
2025-01-17 16:08:03 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#1 started running PipelineTask#1
2025-01-17 16:08:03 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 16:08:03 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:08:03 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG
2025-01-17 16:08:04 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/hUYywwfj4nyZzJW3C3VG
2025-01-17 16:08:04 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 16:08:05 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'model': 'nova-2-general', 'transcriptId': 'f8b4d010-0e83-48db-8073-d55023946fd8', 'language': 'en', 'startedBy': '93c93527-a308-4ee4-88a9-a23cc287296a', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f'}
2025-01-17 16:08:12 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 9ea2cc7b-5d6b-4502-be44-4f7a010d1cdc
2025-01-17 16:08:19 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:08:21 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:08:21 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#1
2025-01-17 16:08:21 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Hello?
2025-01-17 16:08:21 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#1 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x101427560>
                                      └ <function spawn_main at 0x101427560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x101427600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1011a8680>
           └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1011a3ba0>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1011b30e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    └ <function subprocess_started at 0x102141a80>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1020e2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x102140860>
           │       │   └ <uvicorn.server.Server object at 0x1020e2e40>
           │       └ <function run at 0x1014858a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1021617e0>
           │      └ <function Runner.run at 0x101ec5260>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101ec2b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101ec2ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101ec4900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101e39620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=5467, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'user', 'content': 'Enabling transcription....
          │    └ <function LangchainProcessor.process_frame at 0x10a39e480>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Hello?'
          │    └ <function LangchainProcessor._ainvoke at 0x10a3dec00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10a33ad40>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10d05bec0>
                       │    └ <function RunnableSequence.atransform at 0x10a33aca0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10a3394e0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x101ead1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10cffa9b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10cffadf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d1fc150>, 'rec...
                        │    │       └ {'input': 'Hello?'}
                        │    └ <function Runnable.astream at 0x10a3387c0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d1fc150>, 'rec...
                │    │       └ {'input': 'Hello?'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10abfb7e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10a339260>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10cfce7c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10a86a5e0>
                           │       └ <function create_task at 0x101ead1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Hello?'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10abfae80>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 16:08:23 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:08:24 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:08:24 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#2
2025-01-17 16:08:24 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Why isn't this working?
2025-01-17 16:08:24 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#1 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x101427560>
                                      └ <function spawn_main at 0x101427560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x101427600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1011a8680>
           └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1011a3ba0>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1011b30e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>
    │    └ <function subprocess_started at 0x102141a80>
    └ <SpawnProcess name='SpawnProcess-2' parent=74401 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1020e2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x102140860>
           │       │   └ <uvicorn.server.Server object at 0x1020e2e40>
           │       └ <function run at 0x1014858a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1021617e0>
           │      └ <function Runner.run at 0x101ec5260>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101ec2b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1020e3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101ec2ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101ec4900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101e39620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=5643, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'user', 'content': 'Enabling transcription....
          │    └ <function LangchainProcessor.process_frame at 0x10a39e480>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ "Why isn't this working?"
          │    └ <function LangchainProcessor._ainvoke at 0x10a3dec00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10a33ad40>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10cf2b390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10d0849a0>
                       │    └ <function RunnableSequence.atransform at 0x10a33aca0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10a3394e0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x101ead1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10cffa350>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10cffa8a0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d1fd8d0>, 'rec...
                        │    │       └ {'input': "Why isn't this working?"}
                        │    └ <function Runnable.astream at 0x10a3387c0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10d1fd8d0>, 'rec...
                │    │       └ {'input': "Why isn't this working?"}
                │    └ <function BasePromptTemplate.ainvoke at 0x10abfb7e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10a339260>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10cfb2c00>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10d0612a0>
                           │       └ <function create_task at 0x101ead1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': "Why isn't this working?"}
                   │    └ <function BasePromptTemplate._validate_input at 0x10abfae80>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 16:08:27 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:08:39 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:08:39 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:08:41 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:08:41 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:08:41 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:08:41 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:09:01 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:09:01 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:09:01 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/c4z70nESM2T6wDWYuhjL
2025-01-17 16:09:01 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/c4z70nESM2T6wDWYuhjL
2025-01-17 16:09:01 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:09:01 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:09:01 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x120e142f0>
2025-01-17 16:09:01 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/c4z70nESM2T6wDWYuhjL and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiYzR6NzBuRVNNMlQ2d0RXWXVoakwiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE3NDEsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgxNDF9.7YZcCsplhdyV6khir9mOaFJk5DlTBBK3mpjnJcTJrx8
2025-01-17 16:09:01 | INFO     | app.bot.interview_bot:start:237 - Starting interview session in room: https://getroborecruiter.daily.co/c4z70nESM2T6wDWYuhjL
2025-01-17 16:09:01 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:09:01 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:09:01 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:09:02 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:09:02 | ERROR    | app.bot.interview_bot:start:252 - Error during interview session: 'DailyTransport' object has no attribute 'set_audio_output'
2025-01-17 16:09:02 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: 'DailyTransport' object has no attribute 'set_audio_output'
2025-01-17 16:09:02 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: 'DailyTransport' object has no attribute 'set_audio_output'
2025-01-17 16:09:39 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:09:39 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:09:39 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:09:41 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:09:41 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:09:41 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:09:41 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:09:46 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:09:46 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:09:47 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/WI927gr0oIJkoxKdHAo5
2025-01-17 16:09:47 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/WI927gr0oIJkoxKdHAo5
2025-01-17 16:09:47 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:09:47 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:09:47 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x117c102f0>
2025-01-17 16:09:47 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/WI927gr0oIJkoxKdHAo5 and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiV0k5MjdncjBvSUprb3hLZEhBbzUiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE3ODYsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgxODd9.ozScYZtYRDi9Vs3HVfWQx-uQo5nlv1KZUcvl_Z-_UpM
2025-01-17 16:09:47 | INFO     | app.bot.interview_bot:start:237 - Starting interview session in room: https://getroborecruiter.daily.co/WI927gr0oIJkoxKdHAo5
2025-01-17 16:09:47 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:09:47 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:09:47 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:09:47 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:09:47 | DEBUG    | app.bot.interview_bot:_init_tts_service:99 - Initializing Cartesia TTS
2025-01-17 16:09:47 | ERROR    | app.bot.interview_bot:start:251 - Error during interview session: 'CartesiaTTSService' object has no attribute 'connect'
2025-01-17 16:09:47 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: 'CartesiaTTSService' object has no attribute 'connect'
2025-01-17 16:09:47 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: 'CartesiaTTSService' object has no attribute 'connect'
2025-01-17 16:11:13 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:11:14 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:11:14 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:11:15 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:11:15 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:11:15 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:11:15 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:11:26 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:11:27 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:11:27 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:11:28 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:11:28 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:11:28 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:11:28 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:11:31 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:11:31 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:11:32 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/Rm5EsQH9eCDbaTyhUowa
2025-01-17 16:11:32 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/Rm5EsQH9eCDbaTyhUowa
2025-01-17 16:11:32 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:11:32 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:11:32 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x113910440>
2025-01-17 16:11:32 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/Rm5EsQH9eCDbaTyhUowa and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiUm01RXNRSDllQ0RiYVR5aFVvd2EiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE4OTIsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgyOTJ9.85iHAonblNZ45KKb54csCdXE9l2DjH1jewl0cqv_3TI
2025-01-17 16:11:32 | INFO     | app.bot.interview_bot:start:238 - Starting interview session in room: https://getroborecruiter.daily.co/Rm5EsQH9eCDbaTyhUowa
2025-01-17 16:11:32 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:11:32 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:11:32 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:11:32 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:11:32 | ERROR    | app.bot.interview_bot:_setup_pipeline:202 - Error setting up pipeline: LangchainProcessor.__init__() got an unexpected keyword argument 'initial_input'
2025-01-17 16:11:32 | ERROR    | app.bot.interview_bot:start:248 - Error during interview session: LangchainProcessor.__init__() got an unexpected keyword argument 'initial_input'
2025-01-17 16:11:32 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: LangchainProcessor.__init__() got an unexpected keyword argument 'initial_input'
2025-01-17 16:11:32 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: LangchainProcessor.__init__() got an unexpected keyword argument 'initial_input'
2025-01-17 16:12:10 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:12:11 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:11 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:12 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:12:12 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:12 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:12 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:12:15 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:12:15 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:12:16 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/s1mk66Muqb9m1moYo0Qo
2025-01-17 16:12:16 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/s1mk66Muqb9m1moYo0Qo
2025-01-17 16:12:16 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:12:16 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:12:16 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10c584440>
2025-01-17 16:12:16 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/s1mk66Muqb9m1moYo0Qo and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiczFtazY2TXVxYjltMW1vWW8wUW8iLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE5MzYsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgzMzZ9._gqpHp7sMl04yA_UYmHrGjriKwEKbmkt1yHtRlsP7ck
2025-01-17 16:12:16 | INFO     | app.bot.interview_bot:start:236 - Starting interview session in room: https://getroborecruiter.daily.co/s1mk66Muqb9m1moYo0Qo
2025-01-17 16:12:16 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:12:16 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:12:16 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:12:16 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:12:16 | ERROR    | app.bot.interview_bot:_setup_pipeline:200 - Error setting up pipeline: LangchainProcessor.__init__() got an unexpected keyword argument 'history'
2025-01-17 16:12:16 | ERROR    | app.bot.interview_bot:start:246 - Error during interview session: LangchainProcessor.__init__() got an unexpected keyword argument 'history'
2025-01-17 16:12:16 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: LangchainProcessor.__init__() got an unexpected keyword argument 'history'
2025-01-17 16:12:16 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: LangchainProcessor.__init__() got an unexpected keyword argument 'history'
2025-01-17 16:12:25 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:12:26 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:26 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:28 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:12:28 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:28 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:28 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:12:28 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:28 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:29 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:12:29 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:29 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:29 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:12:35 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 16:12:35 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:35 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:38 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:12:38 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:12:38 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:12:38 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:12:39 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:12:39 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:12:39 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:12:39 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:12:39 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 16:12:39 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:12:39 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10cea8440>
2025-01-17 16:12:39 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiakg3MjJlVGRZQzZaMnNBTThEaVUiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTE5NTksImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDgzNTl9.c6WIt_MGTgPXhiCMhJiKz_nTLIEdcNs2tDvczydqalM
2025-01-17 16:12:39 | INFO     | app.bot.interview_bot:start:233 - Starting interview session in room: https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:12:39 | DEBUG    | app.bot.interview_bot:_init_daily_transport:60 - Initializing Daily transport
2025-01-17 16:12:39 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:12:39 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:12:39 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:12:40 | DEBUG    | app.bot.interview_bot:_init_tts_service:99 - Initializing Cartesia TTS
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 16:12:40 | DEBUG    | app.bot.interview_bot:_setup_pipeline:193 - Pipeline setup complete
2025-01-17 16:12:40 | DEBUG    | app.bot.interview_bot:_run_pipeline:210 - Starting pipeline runner
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 16:12:40 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 16:12:40 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 16:12:40 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 16:12:40 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:12:40 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:12:41 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:12:41 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 16:12:42 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'startedBy': 'cd8d5766-647e-4d49-b1b3-030cbf56deca', 'transcriptId': '4e518a5d-5baf-466b-a96a-84ba7f46854d', 'model': 'nova-2-general', 'language': 'en'}
2025-01-17 16:12:50 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 51851673-336d-4539-8cc6-71f0b00969cb
2025-01-17 16:12:55 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:12:55 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:12:55 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 16:12:55 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Hello?
2025-01-17 16:12:56 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x102f4f560>
                                      └ <function spawn_main at 0x102f4f560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x102f4f600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x102cd0680>
           └ <SpawnProcess name='SpawnProcess-8' parent=74977 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x102ccbba0>
    └ <SpawnProcess name='SpawnProcess-8' parent=74977 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x102cdb0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-8' parent=74977 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-8' parent=74977 started>
    │    └ <function subprocess_started at 0x103c69a80>
    └ <SpawnProcess name='SpawnProcess-8' parent=74977 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x103c0ae40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x103c68860>
           │       │   └ <uvicorn.server.Server object at 0x103c0ae40>
           │       └ <function run at 0x102fad8a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x103c897e0>
           │      └ <function Runner.run at 0x1039ed260>
           └ <asyncio.runners.Runner object at 0x103c0bb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1039eab60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x103c0bb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1039eaac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1039ec900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103961620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=286, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'user', 'content': 'Hello?'}])
          │    └ <function LangchainProcessor.process_frame at 0x10c73e520>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x111c6d7f0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Hello?'
          │    └ <function LangchainProcessor._ainvoke at 0x10c786c00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x111c6d7f0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10c6dae80>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x111c6d7f0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x111eea160>
                       │    └ <function RunnableSequence.atransform at 0x10c6dade0>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10c6d9620>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1039d51c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x111f45250>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x111f45030>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x111f7c8a0>, 'rec...
                        │    │       └ {'input': 'Hello?'}
                        │    └ <function Runnable.astream at 0x10c6d8900>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x111f7c8a0>, 'rec...
                │    │       └ {'input': 'Hello?'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10e023ce0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10c6d93a0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c54bac0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x111f982e0>
                           │       └ <function create_task at 0x1039d51c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Hello?'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10e023a60>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 16:13:16 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 16:13:16 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 16:13:16 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 16:13:16 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 16:13:17 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:13:17 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:13:17 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 16:13:17 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:13:17 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 16:13:17 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/jH722eTdYC6Z2sAM8DiU
2025-01-17 16:14:36 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:14:36 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:14:37 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 16:14:37 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 16:14:37 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 16:14:37 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 16:14:43 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:14:43 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:14:44 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:14:44 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:14:44 | INFO     | app.bot.interview_bot:__init__:45 - Initializing Interview Bot
2025-01-17 16:14:44 | DEBUG    | app.bot.interview_bot:__init__:46 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:14:44 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x122a67b60>
2025-01-17 16:14:44 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiMXR4MnpxVjN4RUp4alBQc0xGaEoiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTIwODQsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDg0ODR9.jmxZDv2LGKpbe1_c7egcJcqmNhOTER9mE7rVs0-C2bc
2025-01-17 16:14:44 | INFO     | app.bot.interview_bot:start:235 - Starting interview session in room: https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:14:44 | DEBUG    | app.bot.interview_bot:_init_daily_transport:62 - Initializing Daily transport
2025-01-17 16:14:44 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:14:44 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:14:44 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:14:45 | DEBUG    | app.bot.interview_bot:_init_tts_service:101 - Initializing Cartesia TTS
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 16:14:45 | DEBUG    | app.bot.interview_bot:_setup_pipeline:195 - Pipeline setup complete
2025-01-17 16:14:45 | DEBUG    | app.bot.interview_bot:_run_pipeline:212 - Starting pipeline runner
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 16:14:45 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 16:14:45 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 16:14:45 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 16:14:45 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:14:45 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:14:46 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:14:46 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 16:14:47 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'transcriptId': '78f16767-7a25-41d2-a9e3-7818e481e0f7', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'startedBy': '44ff5285-d9d7-4eaf-9a3c-952f141bbde4', 'language': 'en', 'model': 'nova-2-general'}
2025-01-17 16:14:55 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 6ee769f6-1028-4960-8708-ad795af88fea
2025-01-17 16:15:04 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:15:05 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:15:05 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 16:15:05 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Hello?
2025-01-17 16:15:05 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: 'AIMessage' object has no attribute 'astream'
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104ce7560>
                                      └ <function spawn_main at 0x104ce7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104ce7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x104a68680>
           └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x104a63ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x104a730e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    └ <function subprocess_started at 0x105a01a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1059a2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x105a00860>
           │       │   └ <uvicorn.server.Server object at 0x1059a2e40>
           │       └ <function run at 0x104d458a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x105a217e0>
           │      └ <function Runner.run at 0x105785260>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x105782b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x105782ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x105784900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1056f9620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=494, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'user', 'content': 'Hello?'}])
          │    └ <function LangchainProcessor.process_frame at 0x10eb3a700>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10db21940>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Hello?'
          │    └ <function LangchainProcessor._ainvoke at 0x10eb82ca0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10db21940>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    └ AIMessage(content="Sounds good, let's get started. I'm looking forward to learning more about your background and technical s...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10db21940>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pydantic/main.py", line 891, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
                                 │                                          └ 'astream'
                                 └ AIMessage(content="Sounds good, let's get started. I'm looking forward to learning more about your background and technical s...

AttributeError: 'AIMessage' object has no attribute 'astream'
2025-01-17 16:16:21 | INFO     | pipecat.transports.services.daily:on_participant_left:630 - Participant left 6ee769f6-1028-4960-8708-ad795af88fea
2025-01-17 16:17:06 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 16:17:06 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 16:17:06 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 16:17:06 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 16:17:07 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:17:07 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:17:07 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 16:17:07 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:17:07 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 16:17:07 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/1tx2zqV3xEJxjPPsLFhJ
2025-01-17 16:17:31 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 16:17:31 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 16:17:32 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb
2025-01-17 16:17:32 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb
2025-01-17 16:17:32 | INFO     | app.bot.interview_bot:__init__:45 - Initializing Interview Bot
2025-01-17 16:17:32 | DEBUG    | app.bot.interview_bot:__init__:46 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 16:17:32 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10dbb8910>
2025-01-17 16:17:32 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiSEI0T3FnWlltUHJTN1h0SUtsR2IiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTIyNTIsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNDg2NTJ9.OTqvNdOF4PTjXA29666VAaWW_394wtFtw8Dw7PiVpcs
2025-01-17 16:17:32 | INFO     | app.bot.interview_bot:start:235 - Starting interview session in room: https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb
2025-01-17 16:17:32 | DEBUG    | app.bot.interview_bot:_init_daily_transport:62 - Initializing Daily transport
2025-01-17 16:17:32 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 16:17:32 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 16:17:32 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 16:17:33 | DEBUG    | app.bot.interview_bot:_init_tts_service:101 - Initializing Cartesia TTS
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#1 -> DailyInputTransport#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#1 -> DeepgramSTTService#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#1 -> LLMUserResponseAggregator#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#1 -> LangchainProcessor#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#1 -> CartesiaTTSService#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#1 -> DailyOutputTransport#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#1 -> LLMAssistantResponseAggregator#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#1 -> PipelineSink#1
2025-01-17 16:17:33 | DEBUG    | app.bot.interview_bot:_setup_pipeline:195 - Pipeline setup complete
2025-01-17 16:17:33 | DEBUG    | app.bot.interview_bot:_run_pipeline:212 - Starting pipeline runner
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#1 -> Pipeline#1
2025-01-17 16:17:33 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#1 -> Sink#1
2025-01-17 16:17:33 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#1 started running PipelineTask#1
2025-01-17 16:17:33 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 16:17:33 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:17:33 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb
2025-01-17 16:17:35 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/HB4OqgZYmPrS7XtIKlGb
2025-01-17 16:17:35 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 16:17:35 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'transcriptId': '38fe21c6-7227-465c-80ce-b4c4180ed2a4', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'language': 'en', 'model': 'nova-2-general', 'startedBy': 'e86951ae-786a-4c80-9ad4-0e834abd3d40'}
2025-01-17 16:17:44 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 361b1ce3-9a11-4aaa-b4a9-6a3592c30103
2025-01-17 16:20:13 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:20:17 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:20:17 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#1
2025-01-17 16:20:17 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with So sorry to hear about your brother that passed away. He gets 5 big booms.
2025-01-17 16:20:17 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#1 an unknown error occurred: 'AIMessage' object has no attribute 'astream'
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104ce7560>
                                      └ <function spawn_main at 0x104ce7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104ce7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x104a68680>
           └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x104a63ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x104a730e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    └ <function subprocess_started at 0x105a01a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1059a2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x105a00860>
           │       │   └ <uvicorn.server.Server object at 0x1059a2e40>
           │       └ <function run at 0x104d458a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x105a217e0>
           │      └ <function Runner.run at 0x105785260>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x105782b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x105782ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x105784900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1056f9620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=11987, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'user', 'content': 'Hello?'}, {'role': 'us...
          │    └ <function LangchainProcessor.process_frame at 0x10eb3a700>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'So sorry to hear about your brother that passed away. He gets 5 big booms.'
          │    └ <function LangchainProcessor._ainvoke at 0x10eb82ca0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    └ AIMessage(content="Okay, let's begin the technical interview. I'm looking forward to learning more about your technical skill...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pydantic/main.py", line 891, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
                                 │                                          └ 'astream'
                                 └ AIMessage(content="Okay, let's begin the technical interview. I'm looking forward to learning more about your technical skill...

AttributeError: 'AIMessage' object has no attribute 'astream'
2025-01-17 16:22:33 | WARNING  | pipecat.services.cartesia:_reconnect_websocket:279 - CartesiaTTSService#1 reconnecting (attempt: 1)
2025-01-17 16:22:33 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:22:33 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:22:51 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:22:53 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:22:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#2
2025-01-17 16:22:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Yep.
2025-01-17 16:22:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#1 an unknown error occurred: 'AIMessage' object has no attribute 'astream'
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104ce7560>
                                      └ <function spawn_main at 0x104ce7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104ce7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x104a68680>
           └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x104a63ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x104a730e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>
    │    └ <function subprocess_started at 0x105a01a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=75614 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1059a2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x105a00860>
           │       │   └ <uvicorn.server.Server object at 0x1059a2e40>
           │       └ <function run at 0x104d458a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x105a217e0>
           │      └ <function Runner.run at 0x105785260>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x105782b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1059a3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x105782ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x105784900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1056f9620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=19762, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'user', 'content': 'Hello?'}, {'role': 'us...
          │    └ <function LangchainProcessor.process_frame at 0x10eb3a700>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Yep.'
          │    └ <function LangchainProcessor._ainvoke at 0x10eb82ca0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    └ AIMessage(content="Okay, let's begin the technical interview. I'm looking forward to learning more about your technical skill...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10dbbb390>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pydantic/main.py", line 891, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
                                 │                                          └ 'astream'
                                 └ AIMessage(content="Okay, let's begin the technical interview. I'm looking forward to learning more about your technical skill...

AttributeError: 'AIMessage' object has no attribute 'astream'
2025-01-17 16:26:34 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 16:26:36 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 16:27:33 | WARNING  | pipecat.services.cartesia:_reconnect_websocket:279 - CartesiaTTSService#1 reconnecting (attempt: 2)
2025-01-17 16:27:33 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 16:27:33 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 16:32:34 | ERROR    | pipecat.services.cartesia:_receive_task_handler:298 - CartesiaTTSService#1 error receiving messages: no close frame received or sent
2025-01-17 16:32:34 | ERROR    | pipecat.pipeline.task:_handle_upstream_frame:62 - Error running app: ErrorFrame#0(error: CartesiaTTSService#1 error receiving messages: no close frame received or sent, fatal: True)
2025-01-17 16:32:34 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 16:32:34 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 18:14:54 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:14:54 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:14:56 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:14:56 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:14:56 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:14:56 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:15:04 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 18:15:04 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 18:15:04 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/ZHqKvFrqp4JfB6EETEHS
2025-01-17 18:15:04 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/ZHqKvFrqp4JfB6EETEHS
2025-01-17 18:15:04 | INFO     | app.bot.interview_bot:__init__:55 - Initializing Interview Bot
2025-01-17 18:15:04 | DEBUG    | app.bot.interview_bot:__init__:56 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 18:15:04 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x11cec8590>
2025-01-17 18:15:04 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/ZHqKvFrqp4JfB6EETEHS and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiWkhxS3ZGcnFwNEpmQjZFRVRFSFMiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTkzMDQsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTU3MDR9.M4xZ0nyFiMv-1yBQrEBbdg6iUMp-AQerqks1HHZf76E
2025-01-17 18:15:04 | INFO     | app.bot.interview_bot:start:241 - Starting interview session in room: https://getroborecruiter.daily.co/ZHqKvFrqp4JfB6EETEHS
2025-01-17 18:15:04 | DEBUG    | app.bot.interview_bot:_init_daily_transport:72 - Initializing Daily transport
2025-01-17 18:15:04 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 18:15:04 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 18:15:04 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 18:15:04 | ERROR    | app.bot.interview_bot:start:254 - Error during interview session: 'DailyTransport' object has no attribute 'start'
2025-01-17 18:15:04 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: 'DailyTransport' object has no attribute 'start'
2025-01-17 18:15:04 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: 'DailyTransport' object has no attribute 'start'
2025-01-17 18:16:27 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 18:16:28 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:16:28 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:16:30 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:16:30 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:16:30 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:16:30 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:16:42 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 18:16:42 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 18:16:43 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:16:43 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:16:43 | INFO     | app.bot.interview_bot:__init__:55 - Initializing Interview Bot
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:__init__:56 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 18:16:43 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10e50c590>
2025-01-17 18:16:43 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoic25xUExKRFpNSkxzVWh1bUtsRXMiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTk0MDIsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTU4MDJ9.PdLwpi12I8W5l0UCDqvtW4YWS3Hm9UeY-lR1klXxceA
2025-01-17 18:16:43 | INFO     | app.bot.interview_bot:start:241 - Starting interview session in room: https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:_init_daily_transport:72 - Initializing Daily transport
2025-01-17 18:16:43 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 18:16:43 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 18:16:43 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:start:247 - Daily transport is now active
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:_init_tts_service:111 - Initializing Cartesia TTS
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:_setup_pipeline:201 - Pipeline setup complete
2025-01-17 18:16:43 | DEBUG    | app.bot.interview_bot:_run_pipeline:218 - Starting pipeline runner
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 18:16:43 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 18:16:43 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 18:16:43 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 18:16:43 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 18:16:43 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:16:45 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:16:45 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 18:16:45 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'language': 'en', 'startedBy': '85310328-cf74-4546-af6f-f1703ed2f75a', 'model': 'nova-2-general', 'transcriptId': 'b139754c-c857-49c6-bda4-45a0a96b7e69', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f'}
2025-01-17 18:17:29 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 18:17:29 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 18:17:29 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 18:17:29 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 18:17:29 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 18:17:29 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:17:29 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 18:17:29 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:17:29 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 18:17:29 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/snqPLJDZMJLsUhumKlEs
2025-01-17 18:21:30 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:21:30 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:21:31 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:21:31 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:21:31 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:21:31 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:21:41 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 18:21:42 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:21:42 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:21:43 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:21:43 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:21:43 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:21:43 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:21:55 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 18:21:55 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 18:21:55 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:21:55 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:21:55 | INFO     | app.bot.interview_bot:__init__:55 - Initializing Interview Bot
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:__init__:56 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 18:21:55 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x11a6c8590>
2025-01-17 18:21:55 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiZ3pEQ3RUYVdDMDE5dlp0Q0RzbEQiLCJvIjp0cnVlLCJleHAiOjE3MzcxNTk3MTUsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTYxMTV9.Ab1PqNCh7jYkxJog-BivUkXShsUEto6gYN0OPGCnLow
2025-01-17 18:21:55 | INFO     | app.bot.interview_bot:start:264 - Starting interview session in room: https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:_init_daily_transport:72 - Initializing Daily transport
2025-01-17 18:21:55 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 18:21:55 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 18:21:55 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:start:268 - Daily transport is now active
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:_init_tts_service:111 - Initializing Cartesia TTS
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:_setup_pipeline:201 - Pipeline setup complete
2025-01-17 18:21:55 | DEBUG    | app.bot.interview_bot:_run_pipeline:218 - Starting pipeline runner
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 18:21:55 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 18:21:55 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 18:21:55 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 18:21:56 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 18:21:56 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:21:57 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:21:57 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 18:21:57 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'transcriptId': '6f4771eb-f05f-477b-98be-f5da0a902261', 'startedBy': '892232ff-90b9-4095-8540-81ebd2cf0f13', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'model': 'nova-2-general', 'language': 'en'}
2025-01-17 18:22:03 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 18:22:03 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 18:22:03 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 18:22:03 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 18:22:03 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 18:22:04 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:22:04 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 18:22:04 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:22:04 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 18:22:04 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:233 - Cleaning up pipeline
2025-01-17 18:22:04 | ERROR    | app.bot.interview_bot:_cleanup_pipeline:246 - Error during pipeline cleanup: 'Pipeline' object has no attribute 'processors'
2025-01-17 18:22:04 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/gzDCtTaWC019vZtCDslD
2025-01-17 18:25:15 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:25:15 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:25:17 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:25:17 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:25:17 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:25:17 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:25:24 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 18:25:24 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 18:25:25 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:25 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:25 | INFO     | app.bot.interview_bot:__init__:55 - Initializing Interview Bot
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:__init__:56 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 18:25:25 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x117ec8590>
2025-01-17 18:25:25 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiRWhNd0o5R01JMFR1bFBLVkhXMXciLCJvIjp0cnVlLCJleHAiOjE3MzcxNTk5MjUsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTYzMjV9.as36eMi4s074s17IdTFrpPRdto-8lvsW4QVMPr-njWU
2025-01-17 18:25:25 | INFO     | app.bot.interview_bot:start:265 - Starting interview session in room: https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:_init_daily_transport:72 - Initializing Daily transport
2025-01-17 18:25:25 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 18:25:25 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 18:25:25 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:start:269 - Daily transport is now active
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:_init_tts_service:111 - Initializing Cartesia TTS
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> LLMUserResponseAggregator#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:_setup_pipeline:201 - Pipeline setup complete
2025-01-17 18:25:25 | DEBUG    | app.bot.interview_bot:_run_pipeline:218 - Starting pipeline runner
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 18:25:25 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 18:25:25 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 18:25:25 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 18:25:25 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 18:25:25 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:27 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:27 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 18:25:27 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'language': 'en', 'startedBy': '5a6e3506-1fdb-4b28-bc2f-137c5d016765', 'model': 'nova-2-general', 'transcriptId': '8bfb49c8-6ca9-469c-bbd7-c9b37464d165', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f'}
2025-01-17 18:25:31 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 18:25:31 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 18:25:31 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 18:25:31 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 18:25:32 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 18:25:32 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:32 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 18:25:32 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:25:32 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 18:25:32 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:233 - Cleaning up pipeline
2025-01-17 18:25:32 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:245 - Pipeline cleanup completed
2025-01-17 18:25:32 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/EhMwJ9GMI0TulPKVHW1w
2025-01-17 18:39:34 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:39:34 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:40:46 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:40:46 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:44:39 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:44:39 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:45:20 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:45:20 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:45:25 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:45:25 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:45:54 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:45:54 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:45:58 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:45:58 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:46:06 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:46:06 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:46:57 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:46:57 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:47:16 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:47:16 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:47:37 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:47:37 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:47:44 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:47:44 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:48:10 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:48:10 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:48:13 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:48:13 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:48:40 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:48:40 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:48:47 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:48:47 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:48:52 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:48:52 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:49:04 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:49:04 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:49:09 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:49:09 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:49:29 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:49:29 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:49:43 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:49:43 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:49:54 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:49:54 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:50:05 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:50:05 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:50:42 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:50:42 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:50:47 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:50:47 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:51:04 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:51:04 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:51:13 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:51:13 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:51:21 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:51:21 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:51:31 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:51:31 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:52:05 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:52:05 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:52:49 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:52:49 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:52:55 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:52:55 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:02 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:02 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:09 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:09 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:36 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:36 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:39 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:39 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:44 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:44 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:53:51 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:53:51 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:54:01 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:54:01 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:54:18 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:54:18 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:54:29 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:54:29 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:54:31 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:54:31 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:54:31 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:54:31 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:58:41 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 18:58:42 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:58:42 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:58:43 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:58:43 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:58:43 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:58:43 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:58:57 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 18:58:58 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:58:58 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:58:59 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:58:59 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:58:59 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:58:59 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 18:59:53 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 18:59:54 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:59:54 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:59:55 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 18:59:55 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 18:59:55 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 18:59:55 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:00:06 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:00:06 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:00:08 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/LOebmpDRmAP8MHjYkX9s
2025-01-17 19:00:08 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/LOebmpDRmAP8MHjYkX9s
2025-01-17 19:00:08 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:00:08 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:00:08 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x120105010>
2025-01-17 19:00:08 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/LOebmpDRmAP8MHjYkX9s and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiTE9lYm1wRFJtQVA4TUhqWWtYOXMiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIwMDcsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg0MDh9.wi3fmoewNDJLI_RMNQ59_k2Ow6ADXlHfX9OxvCH5m2I
2025-01-17 19:00:08 | INFO     | app.bot.interview_bot:start:120 - Starting interview session in room: https://getroborecruiter.daily.co/LOebmpDRmAP8MHjYkX9s
2025-01-17 19:00:08 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:00:08 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:00:08 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:00:08 | ERROR    | app.bot.interview_bot:start:138 - Error during interview session: Event handler on_error not registered
2025-01-17 19:00:08 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: Event handler on_error not registered
2025-01-17 19:00:08 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: Event handler on_error not registered
2025-01-17 19:01:41 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 19:01:42 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:01:42 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:01:44 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 19:01:44 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:01:44 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:01:44 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:01:47 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:01:47 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:01:47 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/42lBDlgcyOQUUu800KHw
2025-01-17 19:01:47 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/42lBDlgcyOQUUu800KHw
2025-01-17 19:01:47 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:01:47 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:01:47 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x11cbfd010>
2025-01-17 19:01:47 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/42lBDlgcyOQUUu800KHw and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiNDJsQkRsZ2N5T1FVVXU4MDBLSHciLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIxMDcsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg1MDd9.ZOioqTOuIf5UWYtnyAF62jzYcxxA2Gnjeo_8O4b7Hk8
2025-01-17 19:01:47 | INFO     | app.bot.interview_bot:start:120 - Starting interview session in room: https://getroborecruiter.daily.co/42lBDlgcyOQUUu800KHw
2025-01-17 19:01:47 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:01:47 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:01:47 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:01:47 | ERROR    | app.bot.interview_bot:start:139 - Error during interview session: 'DailyTransport' object has no attribute 'on'
2025-01-17 19:01:47 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: 'DailyTransport' object has no attribute 'on'
2025-01-17 19:01:47 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: 'DailyTransport' object has no attribute 'on'
2025-01-17 19:02:49 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 19:02:50 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:02:50 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:02:51 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 19:02:51 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:02:51 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:02:51 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:02:52 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:02:52 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:02:53 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/HCMPIBzVywglv4g3G6tn
2025-01-17 19:02:53 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/HCMPIBzVywglv4g3G6tn
2025-01-17 19:02:53 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:02:53 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:02:53 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x118afd010>
2025-01-17 19:02:53 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/HCMPIBzVywglv4g3G6tn and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiSENNUElCelZ5d2dsdjRnM0c2dG4iLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIxNzMsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg1NzN9.xJzCwBKDxmMsrM74kh8Df3vWz-lpajEtkTZPrfC65_E
2025-01-17 19:02:53 | INFO     | app.bot.interview_bot:start:120 - Starting interview session in room: https://getroborecruiter.daily.co/HCMPIBzVywglv4g3G6tn
2025-01-17 19:02:53 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:02:53 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:02:53 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:02:53 | ERROR    | app.bot.interview_bot:start:139 - Error during interview session: Event handler on_error not registered
2025-01-17 19:02:53 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: Event handler on_error not registered
2025-01-17 19:02:53 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: Event handler on_error not registered
2025-01-17 19:03:48 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 19:03:49 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:03:49 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:03:51 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 19:03:51 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:03:51 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:03:51 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:03:53 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:03:53 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:03:53 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/vJ9AZeWC3HHYSFl7vaX0
2025-01-17 19:03:53 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/vJ9AZeWC3HHYSFl7vaX0
2025-01-17 19:03:53 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:03:53 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:03:53 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x12aff9010>
2025-01-17 19:03:53 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/vJ9AZeWC3HHYSFl7vaX0 and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoidko5QVplV0MzSEhZU0ZsN3ZhWDAiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIyMzMsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg2MzN9.yZaJV5Z7H_6KBqV4u-OOm65fgUEkEB5LUmXmGgyUVLw
2025-01-17 19:03:53 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/vJ9AZeWC3HHYSFl7vaX0
2025-01-17 19:03:53 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:03:53 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:03:54 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:03:54 | ERROR    | app.bot.interview_bot:start:142 - Error during interview session: Event handler on_error not registered
2025-01-17 19:03:54 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: Event handler on_error not registered
2025-01-17 19:03:54 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: Event handler on_error not registered
2025-01-17 19:04:00 | INFO     | app.main:lifespan:15 - Shutting down Interview Bot API
2025-01-17 19:04:01 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:04:01 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:04:02 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 19:04:02 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:04:02 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:04:02 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:04:04 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:04:04 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:04:04 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:04:04 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:04:04 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:04:04 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:04:04 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x12c9fd010>
2025-01-17 19:04:04 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3 and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiZnkwdHBmYnkxQ2x5VWJFTEJ2czMiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIyNDQsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg2NDV9.eT8byUkm9GCbRb5k0xMzgpeikxaJ8ARTCKj0-N3W8-A
2025-01-17 19:04:04 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:04:04 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:04:04 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:04:05 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:04:05 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:04:05 | DEBUG    | app.bot.services.tts_service:init_tts_service:8 - Initializing Cartesia TTS
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> IdleFrameProcessor#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking IdleFrameProcessor#0 -> LLMUserResponseAggregator#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 19:04:05 | DEBUG    | app.bot.interview_bot:_setup_pipeline:75 - Pipeline setup complete
2025-01-17 19:04:05 | DEBUG    | app.bot.interview_bot:start:138 - Daily transport is now active
2025-01-17 19:04:05 | DEBUG    | app.bot.interview_bot:_run_pipeline:84 - Starting pipeline runner
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 19:04:05 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 19:04:05 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 19:04:05 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 19:04:05 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 19:04:05 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:04:07 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:04:07 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 19:04:07 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'startedBy': 'dab90a9e-3131-4177-984f-09617bb99f9d', 'model': 'nova-2-general', 'transcriptId': '1d354895-716a-455d-a929-66f1b6f8ba3d', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'language': 'en'}
2025-01-17 19:04:07 | INFO     | app.bot.handlers.event_handlers:on_joined:8 - Bot joined room with data: {'participants': {'local': {'media': {'customAudio': {}, 'customVideo': {}, 'screenVideo': {'state': 'off', 'subscribed': 'unsubscribed', 'offReasons': ['user']}, 'screenAudio': {'offReasons': ['user'], 'state': 'off', 'subscribed': 'unsubscribed'}, 'microphone': {'track': {'id': '67dfed86-8875-4260-bc3e-897f6d0cfa74'}, 'state': 'playable', 'subscribed': 'unsubscribed'}, 'camera': {'offReasons': ['user'], 'subscribed': 'unsubscribed', 'state': 'off'}}, 'id': 'dab90a9e-3131-4177-984f-09617bb99f9d', 'info': {'permissions': {'canSend': ['camera', 'microphone', 'screenVideo', 'customVideo', 'customAudio', 'screenAudio'], 'hasPresence': True, 'canAdmin': ['participants', 'streaming', 'transcription']}, 'userName': 'Sarah', 'isLocal': True, 'isOwner': True, 'joinedAt': 1737158645}}}, 'callConfig': {'roomExpiration': 1737160444, 'ejectAtRoomExpiration': True, 'initialCameraEnabled': False, 'initialMicrophoneEnabled': True, 'roomName': 'fy0tpfby1ClyUbELBvs3', 'terseLoggingEnabled': False, 'ejectAtTokenExpiration': False, 'tokenExpiration': 1737162244, 'defaultStreamingEndpoints': [], 'permissionsOnJoin': {'hasPresence': True, 'canAdmin': ['transcription', 'participants', 'streaming'], 'canSend': ['screenVideo', 'camera', 'screenAudio', 'customVideo', 'customAudio', 'microphone']}, 'isOwner': True, 'recordingMode': 'off'}, 'meetingSession': {'id': '772ba348-21a3-44a9-983b-be121676ffec'}}
2025-01-17 19:04:12 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 19:04:12 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:12 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=16, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x12c9c1260>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8c040>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12c9fa2c0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12c9fa2c0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12ad56f40>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4a180>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:17 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#1
2025-01-17 19:04:17 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:17 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=19, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x12cb05a80>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b8d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b8d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c9a9480>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb48580>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:22 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#2
2025-01-17 19:04:22 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:22 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=22, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x128ecaa20>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bad0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bad0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12cb913c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4a500>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:27 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#3
2025-01-17 19:04:27 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:27 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=25, name='LLMMessagesFrame#3', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x128ecaa20>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8c480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12cb8c7b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b750>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b750>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c9a9480>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4a180>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:32 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#4
2025-01-17 19:04:32 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:32 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=28, name='LLMMessagesFrame#4', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x12cb76d40>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b5d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b5d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c98f6c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb48580>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:37 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#5
2025-01-17 19:04:37 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:37 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=31, name='LLMMessagesFrame#5', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11ea23f60>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bbd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bbd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12adae940>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4a500>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:42 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#6
2025-01-17 19:04:42 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:42 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=34, name='LLMMessagesFrame#6', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11ea23f60>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8c6a0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12cb8c480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bdd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24bdd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12ad56440>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4b680>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:47 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#7
2025-01-17 19:04:47 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:47 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=37, name='LLMMessagesFrame#7', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11ea23f60>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8cae0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12cb8c150>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b6d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b6d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c98f800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb48580>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:52 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#8
2025-01-17 19:04:52 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:52 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=40, name='LLMMessagesFrame#8', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x128ecaa20>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8c040>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12c9ca550>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12c9ca550>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12cb2bf80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4a500>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:04:57 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#9
2025-01-17 19:04:57 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:04:57 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=43, name='LLMMessagesFrame#9', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x128ecaa20>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12c9f3ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b5d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b5d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c98f6c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb4ac00>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:05:02 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 19:05:02 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 19:05:02 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 19:05:02 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 19:05:02 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#10
2025-01-17 19:05:02 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:05:02 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x104643560>
                                      └ <function spawn_main at 0x104643560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x104643600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x1043c4680>
           └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x1043bfba0>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1043cf0e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>
    │    └ <function subprocess_started at 0x10535da80>
    └ <SpawnProcess name='SpawnProcess-48' parent=83890 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1052fee40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x10535c860>
           │       │   └ <uvicorn.server.Server object at 0x1052fee40>
           │       └ <function run at 0x1046a18a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x10537d7e0>
           │      └ <function Runner.run at 0x1050e1260>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1050deb60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1052ffb60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1050deac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1050e0900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x105055620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=47, name='LLMMessagesFrame#10', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x12c9794e0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x12c97ac00>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x1299914e0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x12c9fecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11ea23f60>
                       │    └ <function RunnableSequence.atransform at 0x129991440>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x12998bc40>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x1050c91c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x12cb8c7b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x12c9f3cd0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b0d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x12998af20>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x12d24b0d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x12ab1a8e0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x12998b9c0>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x12c98f800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x12cb48580>
                           │       └ <function create_task at 0x1050c91c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x12ab19da0>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:05:02 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 19:05:02 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:05:02 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 19:05:02 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:05:02 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 19:05:02 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:94 - Cleaning up pipeline
2025-01-17 19:05:02 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:104 - Pipeline cleanup completed
2025-01-17 19:05:02 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/fy0tpfby1ClyUbELBvs3
2025-01-17 19:05:10 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:05:10 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:05:10 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/Sp9JsziQt4sj8HAPiOLI
2025-01-17 19:05:10 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/Sp9JsziQt4sj8HAPiOLI
2025-01-17 19:05:10 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:05:10 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:05:10 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x12c9f74d0>
2025-01-17 19:05:10 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/Sp9JsziQt4sj8HAPiOLI and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiU3A5SnN6aVF0NHNqOEhBUGlPTEkiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIzMTAsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg3MTB9.XPUNmzDafcDlo744aZvKvd24l4fSuTNBT0axDLQy_CM
2025-01-17 19:05:10 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/Sp9JsziQt4sj8HAPiOLI
2025-01-17 19:05:10 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:05:10 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:05:10 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:05:10 | ERROR    | app.bot.interview_bot:start:142 - Error during interview session: unable to select virtual speaker device
2025-01-17 19:05:10 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: unable to select virtual speaker device
2025-01-17 19:05:10 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: unable to select virtual speaker device
2025-01-17 19:05:35 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:05:35 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:05:37 | INFO     | app.main:lifespan:11 - Starting Interview Bot API
2025-01-17 19:05:37 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:05:37 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:05:37 | INFO     | app.main:lifespan:13 - Configuration validated successfully
2025-01-17 19:05:43 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:05:43 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:05:44 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:05:44 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:05:44 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:05:44 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:05:44 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10c20d010>
2025-01-17 19:05:44 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiUzVkejlPcXZxVmFmZzBvZGdCUlIiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjIzNDMsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg3NDN9.RMc7RcZo_a7TfTzz-FAe8YGDPR9ktMn22LQH-SHA_Yg
2025-01-17 19:05:44 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:05:44 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:05:44 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:05:44 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:05:44 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:05:44 | DEBUG    | app.bot.services.tts_service:init_tts_service:8 - Initializing Cartesia TTS
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> IdleFrameProcessor#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking IdleFrameProcessor#0 -> LLMUserResponseAggregator#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 19:05:44 | DEBUG    | app.bot.interview_bot:_setup_pipeline:75 - Pipeline setup complete
2025-01-17 19:05:44 | DEBUG    | app.bot.interview_bot:start:138 - Daily transport is now active
2025-01-17 19:05:44 | DEBUG    | app.bot.interview_bot:_run_pipeline:84 - Starting pipeline runner
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 19:05:44 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 19:05:44 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 19:05:44 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 19:05:44 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 19:05:44 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:05:45 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:05:45 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 19:05:46 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'language': 'en', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'startedBy': '9c1ee20b-f97c-4102-bf6d-f57c7c8d2b6e', 'model': 'nova-2-general', 'transcriptId': '0b844079-d6ff-4874-8480-3f1e3000c40d'}
2025-01-17 19:05:46 | INFO     | app.bot.handlers.event_handlers:on_joined:8 - Bot joined room with data: {'participants': {'local': {'media': {'microphone': {'subscribed': 'unsubscribed', 'track': {'id': '3142205a-7e79-4ca9-8d68-f9faf1b0aece'}, 'state': 'playable'}, 'screenAudio': {'offReasons': ['user'], 'subscribed': 'unsubscribed', 'state': 'off'}, 'screenVideo': {'state': 'off', 'offReasons': ['user'], 'subscribed': 'unsubscribed'}, 'customVideo': {}, 'camera': {'state': 'off', 'subscribed': 'unsubscribed', 'offReasons': ['user']}, 'customAudio': {}}, 'id': '9c1ee20b-f97c-4102-bf6d-f57c7c8d2b6e', 'info': {'isLocal': True, 'isOwner': True, 'permissions': {'hasPresence': True, 'canAdmin': ['participants', 'transcription', 'streaming'], 'canSend': ['screenAudio', 'microphone', 'customVideo', 'customAudio', 'camera', 'screenVideo']}, 'userName': 'Sarah', 'joinedAt': 1737158744}}}, 'meetingSession': {'id': '1f11d136-fd4e-4bad-92d4-8d4a796ca47f'}, 'callConfig': {'recordingMode': 'off', 'roomName': 'S5dz9OqvqVafg0odgBRR', 'initialMicrophoneEnabled': True, 'tokenExpiration': 1737162343, 'ejectAtTokenExpiration': False, 'ejectAtRoomExpiration': True, 'defaultStreamingEndpoints': [], 'terseLoggingEnabled': False, 'isOwner': True, 'permissionsOnJoin': {'canAdmin': ['streaming', 'transcription', 'participants'], 'canSend': ['customVideo', 'customAudio', 'camera', 'microphone', 'screenVideo', 'screenAudio'], 'hasPresence': True}, 'roomExpiration': 1737160543, 'initialCameraEnabled': False}}
2025-01-17 19:05:51 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 19:05:51 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:05:51 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=16, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3e0c0>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203bc0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203de0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c20a3f0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c20a3f0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10bd21180>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25e180>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:05:56 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#1
2025-01-17 19:05:56 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:05:56 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=19, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3e0c0>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203bc0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f950>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f950>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c22ffc0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25c580>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:01 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#2
2025-01-17 19:06:01 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:01 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=22, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3df80>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203de0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f950>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f950>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10bd72400>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25e500>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:06 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#3
2025-01-17 19:06:06 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:06 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=25, name='LLMMessagesFrame#3', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10c28ede0>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c2a4370>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c2a46a0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f750>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f750>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c22ffc0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25ec00>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:11 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#4
2025-01-17 19:06:11 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:11 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=28, name='LLMMessagesFrame#4', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3df80>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203bc0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c1dee50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c1dee50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c1f18c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25c580>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:16 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#5
2025-01-17 19:06:16 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:16 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=31, name='LLMMessagesFrame#5', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x107d43e20>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203de0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35ff50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35ff50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10bc66d00>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25e500>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:21 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#6
2025-01-17 19:06:21 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:21 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=34, name='LLMMessagesFrame#6', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3df80>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c2a4590>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c2a46a0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fc50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fc50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10bf5ee80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25f680>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:26 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#7
2025-01-17 19:06:26 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:26 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=37, name='LLMMessagesFrame#7', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3df80>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c2a49d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c2a4370>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f1d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35f1d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c0768c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25c580>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:31 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#8
2025-01-17 19:06:31 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:31 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=40, name='LLMMessagesFrame#8', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10bf3df80>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c2a4040>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fed0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fed0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c147200>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25e500>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:36 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#9
2025-01-17 19:06:36 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:36 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=43, name='LLMMessagesFrame#9', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10c28e980>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203de0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203bc0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fc50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fc50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10bd7c380>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25d7e0>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:41 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#10
2025-01-17 19:06:41 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:06:41 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<string>", line 1, in <module>
    from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=5, pipe_handle=7)
                                      │           └ <function spawn_main at 0x100af7560>
                                      └ <function spawn_main at 0x100af7560>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               │     │   └ 4
               │     └ 7
               └ <function _main at 0x100af7600>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           │    │          └ 4
           │    └ <function BaseProcess._bootstrap at 0x100878680>
           └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x100873ba0>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {'config': <uvicorn.config.Config object at 0x1008830e0>, 'target': <bound method Server.run of <uvicorn.server.Server object...
    │    │        │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    │        │    └ ()
    │    │        └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>
    │    └ <function subprocess_started at 0x101811a80>
    └ <SpawnProcess name='SpawnProcess-1' parent=86995 started>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    │              └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
    └ <bound method Server.run of <uvicorn.server.Server object at 0x1017b2e40>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/uvicorn/server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           │       │   │    │             └ [<socket.socket fd=3, family=2, type=1, proto=0, laddr=('127.0.0.1', 8000)>]
           │       │   │    └ <function Server.serve at 0x101810860>
           │       │   └ <uvicorn.server.Server object at 0x1017b2e40>
           │       └ <function run at 0x100b558a0>
           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object Server.serve at 0x1018317e0>
           │      └ <function Runner.run at 0x101595260>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<Server.serve() running at /Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.1...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x101592b60>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x1017b3b60>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101592ac0>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x101594900>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101509620>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=46, name='LLMMessagesFrame#10', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x10c165620>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x10c1667a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x10b8b53a0>
                       │    └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x10c20ecf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x107d43e20>
                       │    └ <function RunnableSequence.atransform at 0x10b8b5300>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x10b89bb00>
                       └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x10157d1c0>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x10c203de0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x10c203ef0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fe50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x10b89ade0>
                        └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x10c35fe50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x10bd327a0>
                └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x10b89b880>
                 └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x10c17f900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x10c25c580>
                           │       └ <function create_task at 0x10157d1c0>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x10bd31b20>
                   └ ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': list[typing.Annotated[typing.Union[typing.An...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history', 'input'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:06:41 | WARNING  | pipecat.pipeline.runner:_sig_handler:51 - Interruption detected. Canceling runner PipelineRunner#0
2025-01-17 19:06:41 | DEBUG    | pipecat.pipeline.runner:cancel:38 - Canceling runner PipelineRunner#0
2025-01-17 19:06:41 | DEBUG    | pipecat.pipeline.task:cancel:116 - Canceling pipeline task PipelineTask#0
2025-01-17 19:06:41 | DEBUG    | pipecat.services.deepgram:_disconnect:208 - Disconnecting from Deepgram
2025-01-17 19:06:42 | DEBUG    | pipecat.services.cartesia:_disconnect_websocket:215 - Disconnecting from Cartesia
2025-01-17 19:06:42 | INFO     | pipecat.transports.services.daily:leave:435 - Leaving https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:06:42 | DEBUG    | pipecat.transports.services.daily:on_transcription_stopped:643 - Transcription stopped
2025-01-17 19:06:42 | INFO     | pipecat.transports.services.daily:leave:443 - Left https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:06:42 | DEBUG    | pipecat.pipeline.runner:run:31 - Runner PipelineRunner#0 finished running PipelineTask#0
2025-01-17 19:06:42 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:94 - Cleaning up pipeline
2025-01-17 19:06:42 | DEBUG    | app.bot.interview_bot:_cleanup_pipeline:104 - Pipeline cleanup completed
2025-01-17 19:06:42 | INFO     | app.api.endpoints.rooms:start_bot_background:18 - Bot started successfully in room: https://getroborecruiter.daily.co/S5dz9OqvqVafg0odgBRR
2025-01-17 19:06:49 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:06:49 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:06:50 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/6U4eRDoKvvNLFbrpP3I4
2025-01-17 19:06:50 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/6U4eRDoKvvNLFbrpP3I4
2025-01-17 19:06:50 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:06:50 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:06:50 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10c207b10>
2025-01-17 19:06:50 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/6U4eRDoKvvNLFbrpP3I4 and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiNlU0ZVJEb0t2dk5MRmJycFAzSTQiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjI0MDksImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg4MTB9.RETvYibuKxWT3z9e4v1T29SfxQnvV0bQU-4Fvtwcjos
2025-01-17 19:06:50 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/6U4eRDoKvvNLFbrpP3I4
2025-01-17 19:06:50 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:06:50 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:06:50 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:06:50 | ERROR    | app.bot.interview_bot:start:142 - Error during interview session: unable to select virtual speaker device
2025-01-17 19:06:50 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: unable to select virtual speaker device
2025-01-17 19:06:50 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: unable to select virtual speaker device
2025-01-17 19:06:54 | INFO     | app.api.endpoints.rooms:create_room:25 - Received request to create new interview room
2025-01-17 19:06:54 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:06:54 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/7aZfNl8LQgQ5XJxmQWRk
2025-01-17 19:06:54 | INFO     | app.api.endpoints.rooms:create_room:31 - Room created successfully: https://getroborecruiter.daily.co/7aZfNl8LQgQ5XJxmQWRk
2025-01-17 19:06:54 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:06:54 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:06:54 | INFO     | app.services.bot:start_bot:17 - Created bot: <app.bot.interview_bot.InterviewBot object at 0x10c206490>
2025-01-17 19:06:54 | INFO     | app.services.bot:start_bot:19 - Starting bot with room_url: https://getroborecruiter.daily.co/7aZfNl8LQgQ5XJxmQWRk and token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyIjoiN2FaZk5sOExRZ1E1WEp4bVFXUmsiLCJvIjp0cnVlLCJleHAiOjE3MzcxNjI0MTQsImQiOiJiZjExYzFlMS04MzEzLTQ1ZjMtYTM3OC0xOTU4MDAwM2M0ZjgiLCJpYXQiOjE3MzcxNTg4MTR9.dUKxF809ODJ9YYl_6lgygeEt5GjAEeHNijecqULvDlo
2025-01-17 19:06:54 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/7aZfNl8LQgQ5XJxmQWRk
2025-01-17 19:06:54 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:06:54 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:06:54 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:06:54 | ERROR    | app.bot.interview_bot:start:142 - Error during interview session: unable to select virtual speaker device
2025-01-17 19:06:54 | ERROR    | app.services.bot:start_bot:23 - Failed to start bot: unable to select virtual speaker device
2025-01-17 19:06:54 | ERROR    | app.api.endpoints.rooms:start_bot_background:20 - Error starting bot: unable to select virtual speaker device
2025-01-17 19:18:09 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:18:09 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:20:08 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:20:08 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:20:08 | INFO     | __main__:<module>:58 - Starting manual bot test...
2025-01-17 19:20:08 | INFO     | __main__:test_interview_bot:29 - Creating Daily room...
2025-01-17 19:20:08 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:20:10 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/IUtb7B0XO8YcAd9GEHyA
2025-01-17 19:20:10 | INFO     | __main__:test_interview_bot:34 - Room created: https://getroborecruiter.daily.co/IUtb7B0XO8YcAd9GEHyA
2025-01-17 19:20:10 | INFO     | __main__:test_interview_bot:37 - Initializing bot...
2025-01-17 19:20:10 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:20:10 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:20:10 | INFO     | __main__:test_interview_bot:41 - Starting bot...
2025-01-17 19:20:10 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/IUtb7B0XO8YcAd9GEHyA
2025-01-17 19:20:10 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:20:10 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:20:10 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:20:10 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:20:10 | DEBUG    | app.bot.services.tts_service:init_tts_service:8 - Initializing Cartesia TTS
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> IdleFrameProcessor#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking IdleFrameProcessor#0 -> LLMUserResponseAggregator#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> LangchainProcessor#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LangchainProcessor#0 -> CartesiaTTSService#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 19:20:10 | DEBUG    | app.bot.interview_bot:_setup_pipeline:75 - Pipeline setup complete
2025-01-17 19:20:10 | DEBUG    | app.bot.interview_bot:start:138 - Daily transport is now active
2025-01-17 19:20:10 | DEBUG    | app.bot.interview_bot:_run_pipeline:84 - Starting pipeline runner
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 19:20:10 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 19:20:10 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 19:20:10 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 19:20:10 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 19:20:10 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/IUtb7B0XO8YcAd9GEHyA
2025-01-17 19:20:13 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/IUtb7B0XO8YcAd9GEHyA
2025-01-17 19:20:13 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 19:20:13 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'language': 'en', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'model': 'nova-2-general', 'startedBy': '371900de-e5ee-41c1-b291-d6aa5efc163d', 'transcriptId': '28f5e1bf-2959-4b0c-9d03-316587900e31'}
2025-01-17 19:20:13 | INFO     | app.bot.handlers.event_handlers:on_joined:8 - Bot joined room with data: {'meetingSession': {'id': '98eb43f2-786a-4aba-8dbd-d81edbbfd5a0'}, 'callConfig': {'ejectAtRoomExpiration': True, 'initialMicrophoneEnabled': True, 'permissionsOnJoin': {'canAdmin': ['streaming', 'transcription', 'participants'], 'canSend': ['customAudio', 'camera', 'screenVideo', 'microphone', 'screenAudio', 'customVideo'], 'hasPresence': True}, 'roomName': 'IUtb7B0XO8YcAd9GEHyA', 'recordingMode': 'off', 'terseLoggingEnabled': False, 'roomExpiration': 1737161408, 'tokenExpiration': 1737163210, 'ejectAtTokenExpiration': False, 'initialCameraEnabled': False, 'isOwner': True, 'defaultStreamingEndpoints': []}, 'participants': {'local': {'info': {'isOwner': True, 'permissions': {'hasPresence': True, 'canSend': ['microphone', 'camera', 'screenVideo', 'screenAudio', 'customVideo', 'customAudio'], 'canAdmin': ['streaming', 'participants', 'transcription']}, 'userName': 'Sarah', 'joinedAt': 1737159610, 'isLocal': True}, 'media': {'customAudio': {}, 'customVideo': {}, 'microphone': {'subscribed': 'unsubscribed', 'track': {'id': '1ef0d5d5-6ed9-4e57-9e9e-2a1b650027cc'}, 'state': 'playable'}, 'screenAudio': {'offReasons': ['user'], 'state': 'off', 'subscribed': 'unsubscribed'}, 'screenVideo': {'offReasons': ['user'], 'state': 'off', 'subscribed': 'unsubscribed'}, 'camera': {'state': 'off', 'offReasons': ['user'], 'subscribed': 'unsubscribed'}}, 'id': '371900de-e5ee-41c1-b291-d6aa5efc163d'}}}
2025-01-17 19:20:18 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#0
2025-01-17 19:20:18 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:18 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=16, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a3e20>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464260>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11a454c30>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11a454c30>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443840>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:23 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#1
2025-01-17 19:20:23 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:23 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=19, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464590>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3708c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a441e00>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:28 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#2
2025-01-17 19:20:28 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:28 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=22, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464260>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93df50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93df50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:33 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#3
2025-01-17 19:20:33 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:33 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=25, name='LLMMessagesFrame#3', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4737e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464ae0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ed50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ed50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d15c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443d80>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:38 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#4
2025-01-17 19:20:38 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:38 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=28, name='LLMMessagesFrame#4', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a405760>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464590>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e2d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e2d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443840>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:43 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#5
2025-01-17 19:20:43 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:43 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=31, name='LLMMessagesFrame#5', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a405760>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464260>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e550>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e550>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a4769c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:48 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#6
2025-01-17 19:20:48 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:48 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=34, name='LLMMessagesFrame#6', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464590>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464480>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ddd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ddd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x1188eaec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a47cd60>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#7
2025-01-17 19:20:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=37, name='LLMMessagesFrame#7', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a405760>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a464bf0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464260>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ead0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ead0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119a3cec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443840>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:20:58 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#8
2025-01-17 19:20:58 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:20:58 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=40, name='LLMMessagesFrame#8', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464ae0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93f9d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93f9d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443d80>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:03 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#9
2025-01-17 19:21:03 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:03 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=43, name='LLMMessagesFrame#9', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ar...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a4503c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:08 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#10
2025-01-17 19:21:08 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:08 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=46, name='LLMMessagesFrame#10', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a405760>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93fb50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93fb50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a4769c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443840>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:13 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#11
2025-01-17 19:21:13 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:13 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=49, name='LLMMessagesFrame#11', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e150>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e150>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443d80>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:18 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#12
2025-01-17 19:21:18 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:18 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=52, name='LLMMessagesFrame#12', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a405760>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93de50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93de50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a450280>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a47c820>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:23 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#13
2025-01-17 19:21:23 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:23 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=55, name='LLMMessagesFrame#13', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1780>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a47c900>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:28 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#14
2025-01-17 19:21:28 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:28 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=58, name='LLMMessagesFrame#14', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ef50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ef50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a440580>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:33 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#15
2025-01-17 19:21:33 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:33 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=61, name='LLMMessagesFrame#15', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ea50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ea50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:38 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#16
2025-01-17 19:21:38 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:38 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=64, name='LLMMessagesFrame#16', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d950>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d950>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:43 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#17
2025-01-17 19:21:43 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:43 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=67, name='LLMMessagesFrame#17', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d7d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d7d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a450280>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:48 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#18
2025-01-17 19:21:48 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:48 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=70, name='LLMMessagesFrame#18', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ea50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ea50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1780>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#19
2025-01-17 19:21:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=73, name='LLMMessagesFrame#19', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ef50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ef50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:21:58 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#20
2025-01-17 19:21:58 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:21:58 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=76, name='LLMMessagesFrame#20', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a473240>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e3d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e3d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3df900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:03 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#21
2025-01-17 19:22:03 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:03 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=79, name='LLMMessagesFrame#21', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:08 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#22
2025-01-17 19:22:08 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:08 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=82, name='LLMMessagesFrame#22', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e9d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e9d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a450280>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:13 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#23
2025-01-17 19:22:13 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:13 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=85, name='LLMMessagesFrame#23', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e0d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e0d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1780>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:18 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#24
2025-01-17 19:22:18 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:18 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=88, name='LLMMessagesFrame#24', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6fd80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:23 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#25
2025-01-17 19:22:23 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:23 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=91, name='LLMMessagesFrame#25', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e750>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e750>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:28 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#26
2025-01-17 19:22:28 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:28 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=94, name='LLMMessagesFrame#26', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d7d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d7d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:33 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#27
2025-01-17 19:22:33 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:33 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=97, name='LLMMessagesFrame#27', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they a...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93f750>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93f750>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a476b40>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:38 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#28
2025-01-17 19:22:38 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:38 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=100, name='LLMMessagesFrame#28', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e050>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e050>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:43 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#29
2025-01-17 19:22:43 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:43 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=103, name='LLMMessagesFrame#29', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ddd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ddd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6fd80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:48 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#30
2025-01-17 19:22:48 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:48 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=106, name='LLMMessagesFrame#30', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93da50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93da50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#31
2025-01-17 19:22:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=109, name='LLMMessagesFrame#31', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e5d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e5d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:22:58 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#32
2025-01-17 19:22:58 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:22:58 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=112, name='LLMMessagesFrame#32', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dd50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dd50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3df900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:03 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#33
2025-01-17 19:23:03 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:03 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=115, name='LLMMessagesFrame#33', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e050>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e050>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1780>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:08 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#34
2025-01-17 19:23:08 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:08 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=118, name='LLMMessagesFrame#34', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e850>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e850>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6fd80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:13 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#35
2025-01-17 19:23:13 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:13 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=121, name='LLMMessagesFrame#35', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ecd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ecd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:18 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#36
2025-01-17 19:23:18 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:18 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=124, name='LLMMessagesFrame#36', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93fb50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93fb50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:23 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#37
2025-01-17 19:23:23 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:23 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=127, name='LLMMessagesFrame#37', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e350>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e350>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3df900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:28 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#38
2025-01-17 19:23:28 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:28 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=130, name='LLMMessagesFrame#38', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e3d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e3d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x1188eaec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:33 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#39
2025-01-17 19:23:33 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:33 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=133, name='LLMMessagesFrame#39', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93eb50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93eb50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a476b40>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:38 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#40
2025-01-17 19:23:38 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:38 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=136, name='LLMMessagesFrame#40', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e350>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e350>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:43 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#41
2025-01-17 19:23:43 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:43 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=139, name='LLMMessagesFrame#41', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:48 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#42
2025-01-17 19:23:48 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:48 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=142, name='LLMMessagesFrame#42', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a4054e0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93df50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93df50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x1188eaec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#43
2025-01-17 19:23:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=145, name='LLMMessagesFrame#43', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93efd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93efd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119a3cec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:23:58 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#44
2025-01-17 19:23:58 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:23:58 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=148, name='LLMMessagesFrame#44', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e150>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e150>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a450280>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:03 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#45
2025-01-17 19:24:03 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:03 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=151, name='LLMMessagesFrame#45', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93d6d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:08 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#46
2025-01-17 19:24:08 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:08 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=154, name='LLMMessagesFrame#46', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e2d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e2d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6fd80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:13 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#47
2025-01-17 19:24:13 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:13 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=157, name='LLMMessagesFrame#47', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e550>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e550>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x1188eaec0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:18 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#48
2025-01-17 19:24:18 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:18 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=160, name='LLMMessagesFrame#48', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x11a3a1bc0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dad0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dad0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6d800>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:23 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#49
2025-01-17 19:24:23 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:23 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=163, name='LLMMessagesFrame#49', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ead0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ead0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:28 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#50
2025-01-17 19:24:28 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:28 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=166, name='LLMMessagesFrame#50', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ec50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ec50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3df900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:33 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#51
2025-01-17 19:24:33 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:33 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=169, name='LLMMessagesFrame#51', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93e4d0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x119d6fd80>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:38 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#52
2025-01-17 19:24:38 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:38 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=172, name='LLMMessagesFrame#52', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:43 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#53
2025-01-17 19:24:43 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:43 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=175, name='LLMMessagesFrame#53', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3fd1c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:48 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#54
2025-01-17 19:24:48 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:48 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=178, name='LLMMessagesFrame#54', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dcd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3d1840>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:53 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#55
2025-01-17 19:24:53 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:53 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=181, name='LLMMessagesFrame#55', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ee50>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93ee50>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a3df900>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443140>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:24:58 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#56
2025-01-17 19:24:58 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:24:58 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=184, name='LLMMessagesFrame#56', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x109fe53a0>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a4649d0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dfd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dfd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a476b40>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a443bc0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:25:03 | DEBUG    | pipecat.processors.frameworks.langchain:process_frame:44 - Got transcription frame LLMMessagesFrame#57
2025-01-17 19:25:03 | DEBUG    | pipecat.processors.frameworks.langchain:_ainvoke:62 - Invoking chain with Ask the user if they are still there and try to prompt for some input.
2025-01-17 19:25:03 | ERROR    | pipecat.processors.frameworks.langchain:_ainvoke:73 - LangchainProcessor#0 an unknown error occurred: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x10305e2a0>
    │       └ <function run at 0x103d35300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x11a267d00>
           │      └ <function Runner.run at 0x103df2a20>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x103df04a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11a23a7b0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103df0400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103df2200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x103764040>
    └ <Handle Task.task_wakeup()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle Task.task_wakeup()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle Task.task_wakeup()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle Task.task_wakeup()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=187, name='LLMMessagesFrame#57', pts=None, messages=[{'role': 'system', 'content': 'Ask the user if they ...
          │    └ <function LangchainProcessor.process_frame at 0x11a3a2160>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 47, in process_frame
    await self._ainvoke(text.strip())
          │    │        │    └ <method 'strip' of 'str' objects>
          │    │        └ 'Ask the user if they are still there and try to prompt for some input.'
          │    └ <function LangchainProcessor._ainvoke at 0x11a3a22a0>
          └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frameworks/langchain.py", line 65, in _ainvoke
    async for token in self._chain.astream(
                       │    │      └ <function RunnableSequence.astream at 0x118aba340>
                       │    └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
                       └ <pipecat.processors.frameworks.langchain.LangchainProcessor object at 0x11a408980>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3430, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
                       │    │          │              │         └ {}
                       │    │          │              └ {'configurable': {'session_id': None}}
                       │    │          └ <function RunnableSequence.astream.<locals>.input_aiter at 0x10fd64220>
                       │    └ <function RunnableSequence.atransform at 0x118aba2a0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3413, in atransform
    async for chunk in self._atransform_stream_with_config(
                       │    └ <function Runnable._atransform_stream_with_config at 0x118ab8ae0>
                       └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 2301, in _atransform_stream_with_config
    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
                          │       └ <function create_task at 0x103d6fc40>
                          └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3383, in _atransform
    async for output in final_pipeline:
                        └ <async_generator object Runnable.atransform at 0x11a4647b0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1453, in atransform
    async for ichunk in input:
                        └ <async_generator object Runnable.atransform at 0x11a464e10>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1471, in atransform
    async for output in self.astream(final, config, **kwargs):
                        │    │       │      │         └ {}
                        │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dfd0>, 'rec...
                        │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                        │    └ <function Runnable.astream at 0x118aa7d80>
                        └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1016, in astream
    yield await self.ainvoke(input, config, **kwargs)
                │    │       │      │         └ {}
                │    │       │      └ {'tags': [], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.AsyncCallbackManager object at 0x11d93dfd0>, 'rec...
                │    │       └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                │    └ <function BasePromptTemplate.ainvoke at 0x1198eb6a0>
                └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 233, in ainvoke
    return await self._acall_with_config(
                 │    └ <function Runnable._acall_with_config at 0x118ab8860>
                 └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 1976, in _acall_with_config
    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
                           │       │           │             └ <_contextvars.Context object at 0x11a32e4c0>
                           │       │           └ <coroutine object BasePromptTemplate._aformat_prompt_with_error_handling at 0x11a4418c0>
                           │       └ <function create_task at 0x103d6fc40>
                           └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 188, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
                   │    │               └ {'input': 'Ask the user if they are still there and try to prompt for some input.'}
                   │    └ <function BasePromptTemplate._validate_input at 0x1198eaa20>
                   └ ChatPromptTemplate(input_variables=['history'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[l...
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/langchain_core/prompts/base.py", line 176, in _validate_input
    raise KeyError(

KeyError: "Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['history'] Received: ['input']\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-01-17 19:25:11 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:25:11 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:25:11 | INFO     | __main__:<module>:58 - Starting manual bot test...
2025-01-17 19:25:11 | INFO     | __main__:test_interview_bot:29 - Creating Daily room...
2025-01-17 19:25:11 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:25:12 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/MV86JYZddRoGY3aexrdc
2025-01-17 19:25:12 | INFO     | __main__:test_interview_bot:34 - Room created: https://getroborecruiter.daily.co/MV86JYZddRoGY3aexrdc
2025-01-17 19:25:12 | INFO     | __main__:test_interview_bot:37 - Initializing bot...
2025-01-17 19:25:12 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:25:12 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:25:12 | INFO     | __main__:test_interview_bot:41 - Starting bot...
2025-01-17 19:25:12 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/MV86JYZddRoGY3aexrdc
2025-01-17 19:25:12 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:25:12 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:25:12 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:25:12 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:25:12 | ERROR    | app.bot.interview_bot:_setup_pipeline:79 - Error setting up pipeline: ChatPromptTemplate.from_messages() got an unexpected keyword argument 'input_variables'
2025-01-17 19:25:12 | ERROR    | app.bot.interview_bot:start:142 - Error during interview session: ChatPromptTemplate.from_messages() got an unexpected keyword argument 'input_variables'
2025-01-17 19:25:12 | ERROR    | __main__:test_interview_bot:52 - Error during test: ChatPromptTemplate.from_messages() got an unexpected keyword argument 'input_variables'
2025-01-17 19:25:12 | INFO     | __main__:test_interview_bot:54 - Test complete
2025-01-17 19:29:12 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:29:12 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:29:12 | INFO     | __main__:<module>:58 - Starting manual bot test...
2025-01-17 19:29:12 | INFO     | __main__:test_interview_bot:29 - Creating Daily room...
2025-01-17 19:29:12 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:29:13 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/uMZqNtQHNwlwvrMOv8yR
2025-01-17 19:29:13 | INFO     | __main__:test_interview_bot:34 - Room created: https://getroborecruiter.daily.co/uMZqNtQHNwlwvrMOv8yR
2025-01-17 19:29:13 | INFO     | __main__:test_interview_bot:37 - Initializing bot...
2025-01-17 19:29:13 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:29:13 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:29:13 | INFO     | __main__:test_interview_bot:41 - Starting bot...
2025-01-17 19:29:13 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/uMZqNtQHNwlwvrMOv8yR
2025-01-17 19:29:13 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:29:13 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:29:13 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:29:13 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:29:13 | DEBUG    | app.bot.services.tts_service:init_tts_service:8 - Initializing Cartesia TTS
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> IdleFrameProcessor#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking IdleFrameProcessor#0 -> LLMUserResponseAggregator#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> AnthropicLLMService#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking AnthropicLLMService#0 -> CartesiaTTSService#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 19:29:13 | DEBUG    | app.bot.interview_bot:_setup_pipeline:75 - Pipeline setup complete
2025-01-17 19:29:13 | DEBUG    | app.bot.interview_bot:start:138 - Daily transport is now active
2025-01-17 19:29:13 | DEBUG    | app.bot.interview_bot:_run_pipeline:84 - Starting pipeline runner
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 19:29:13 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 19:29:13 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 19:29:13 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 19:29:13 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 19:29:13 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/uMZqNtQHNwlwvrMOv8yR
2025-01-17 19:29:14 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/uMZqNtQHNwlwvrMOv8yR
2025-01-17 19:29:14 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 19:29:15 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'startedBy': '8bf3d245-6996-4e77-9a65-278b7fee4d98', 'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'language': 'en', 'transcriptId': 'e277a4b3-aa2e-466e-8ae0-984b5918a7ee', 'model': 'nova-2-general'}
2025-01-17 19:29:15 | INFO     | app.bot.handlers.event_handlers:on_joined:8 - Bot joined room with data: {'meetingSession': {'id': '7ffcdec7-3add-42b7-97ca-f18282325705'}, 'callConfig': {'tokenExpiration': 1737163752, 'ejectAtRoomExpiration': True, 'initialCameraEnabled': False, 'initialMicrophoneEnabled': True, 'ejectAtTokenExpiration': False, 'permissionsOnJoin': {'canSend': ['microphone', 'screenAudio', 'customVideo', 'customAudio', 'screenVideo', 'camera'], 'canAdmin': ['participants', 'streaming', 'transcription'], 'hasPresence': True}, 'roomExpiration': 1737161952, 'roomName': 'uMZqNtQHNwlwvrMOv8yR', 'isOwner': True, 'defaultStreamingEndpoints': [], 'terseLoggingEnabled': False, 'recordingMode': 'off'}, 'participants': {'local': {'id': '8bf3d245-6996-4e77-9a65-278b7fee4d98', 'info': {'isOwner': True, 'permissions': {'canSend': ['customAudio', 'camera', 'screenVideo', 'screenAudio', 'customVideo', 'microphone'], 'hasPresence': True, 'canAdmin': ['transcription', 'streaming', 'participants']}, 'joinedAt': 1737160153, 'userName': 'Sarah', 'isLocal': True}, 'media': {'screenAudio': {'subscribed': 'unsubscribed', 'offReasons': ['user'], 'state': 'off'}, 'screenVideo': {'state': 'off', 'offReasons': ['user'], 'subscribed': 'unsubscribed'}, 'customVideo': {}, 'camera': {'offReasons': ['user'], 'state': 'off', 'subscribed': 'unsubscribed'}, 'microphone': {'state': 'playable', 'subscribed': 'unsubscribed', 'track': {'id': '756a011b-2962-4a89-94a1-e527bdfae4d7'}}, 'customAudio': {}}}}}
2025-01-17 19:29:20 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:29:21 | ERROR    | pipecat.services.anthropic:_process_context:258 - AnthropicLLMService#0 exception: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x102a7ccc0>
    │       └ <function run at 0x103699300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x114a10940>
           │      └ <function Runner.run at 0x103756a20>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1037544a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103754400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103756200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1030c8040>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x1159bb340>()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle <_asyncio.TaskStepMethWrapper object at 0x1159bb340>()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle <_asyncio.TaskStepMethWrapper object at 0x1159bb340>()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x1159bb340>()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=16, name='LLMMessagesFrame#0', pts=None, messages=[{'role': 'user', 'content': 'Ask the user if they are ...
          │    └ <function AnthropicLLMService.process_frame at 0x1158a1da0>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 297, in process_frame
    await self._process_context(context)
          │    │                └ <pipecat.services.anthropic.AnthropicLLMContext object at 0x1159ce900>
          │    └ <function AnthropicLLMService._process_context at 0x1158a1d00>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 178, in _process_context
    response = await api_call(**params)
                     │          └ {'tools': [], 'system': NOT_GIVEN, 'messages': [{'role': 'user', 'content': 'Ask the user if they are still there and try to ...
                     └ <bound method AsyncMessages.create of <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py", line 2051, in create
    return await self._post(
                 │    └ <bound method AsyncAPIClient.post of <anthropic.AsyncAnthropic object at 0x115915010>>
                 └ <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1842, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
                 │    │       │        │            │                  └ anthropic.AsyncStream[typing.Annotated[typing.Union[anthropic.types.raw_message_start_event.RawMessageStartEvent, anthropic.t...
                 │    │       │        │            └ True
                 │    │       │        └ FinalRequestOptions(method='post', url='/v1/messages', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=600, file...
                 │    │       └ <class 'anthropic.types.message.Message'>
                 │    └ <function AsyncAPIClient.request at 0x11567c2c0>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1536, in request
    return await self._request(
                 │    └ <function AsyncAPIClient._request at 0x11567c360>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1637, in _request
    raise self._make_status_error_from_response(err.response) from None
          │    └ <function BaseClient._make_status_error_from_response at 0x115679580>
          └ <anthropic.AsyncAnthropic object at 0x115915010>

anthropic.AuthenticationError: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
2025-01-17 19:29:25 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:29:25 | ERROR    | pipecat.services.anthropic:_process_context:258 - AnthropicLLMService#0 exception: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x102a7ccc0>
    │       └ <function run at 0x103699300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x114a10940>
           │      └ <function Runner.run at 0x103756a20>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1037544a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103754400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103756200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1030c8040>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b10790>()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b10790>()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b10790>()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b10790>()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=19, name='LLMMessagesFrame#1', pts=None, messages=[{'role': 'user', 'content': 'Ask the user if they are ...
          │    └ <function AnthropicLLMService.process_frame at 0x1158a1da0>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 297, in process_frame
    await self._process_context(context)
          │    │                └ <pipecat.services.anthropic.AnthropicLLMContext object at 0x1159e4910>
          │    └ <function AnthropicLLMService._process_context at 0x1158a1d00>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 178, in _process_context
    response = await api_call(**params)
                     │          └ {'tools': [], 'system': NOT_GIVEN, 'messages': [{'role': 'user', 'content': 'Ask the user if they are still there and try to ...
                     └ <bound method AsyncMessages.create of <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py", line 2051, in create
    return await self._post(
                 │    └ <bound method AsyncAPIClient.post of <anthropic.AsyncAnthropic object at 0x115915010>>
                 └ <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1842, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
                 │    │       │        │            │                  └ anthropic.AsyncStream[typing.Annotated[typing.Union[anthropic.types.raw_message_start_event.RawMessageStartEvent, anthropic.t...
                 │    │       │        │            └ True
                 │    │       │        └ FinalRequestOptions(method='post', url='/v1/messages', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=600, file...
                 │    │       └ <class 'anthropic.types.message.Message'>
                 │    └ <function AsyncAPIClient.request at 0x11567c2c0>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1536, in request
    return await self._request(
                 │    └ <function AsyncAPIClient._request at 0x11567c360>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1637, in _request
    raise self._make_status_error_from_response(err.response) from None
          │    └ <function BaseClient._make_status_error_from_response at 0x115679580>
          └ <anthropic.AsyncAnthropic object at 0x115915010>

anthropic.AuthenticationError: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
2025-01-17 19:29:30 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:29:30 | ERROR    | pipecat.services.anthropic:_process_context:258 - AnthropicLLMService#0 exception: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
Traceback (most recent call last):

  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code

  File "/Users/benjamingardiner/talentora-interviewbot/tests/test_bot_manual.py", line 59, in <module>
    asyncio.run(test_interview_bot())
    │       │   └ <function test_interview_bot at 0x102a7ccc0>
    │       └ <function run at 0x103699300>
    └ <module 'asyncio' from '/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyn...

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 194, in run
    return runner.run(main)
           │      │   └ <coroutine object test_interview_bot at 0x114a10940>
           │      └ <function Runner.run at 0x103756a20>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<test_interview_bot() running at /Users/benjamingardiner/talentora-interviewbot/tests/test_b...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1037544a0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11561f0e0>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 707, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x103754400>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 678, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x103756200>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 2033, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1030c8040>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b119c0>()>

  File "/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b119c0>()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b119c0>()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle <_asyncio.TaskStepMethWrapper object at 0x118b119c0>()>

  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/processors/frame_processor.py", line 293, in __input_frame_task_handler
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ LLMMessagesFrame(id=22, name='LLMMessagesFrame#2', pts=None, messages=[{'role': 'user', 'content': 'Ask the user if they are ...
          │    └ <function AnthropicLLMService.process_frame at 0x1158a1da0>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 297, in process_frame
    await self._process_context(context)
          │    │                └ <pipecat.services.anthropic.AnthropicLLMContext object at 0x1159e5590>
          │    └ <function AnthropicLLMService._process_context at 0x1158a1d00>
          └ <pipecat.services.anthropic.AnthropicLLMService object at 0x115914c20>
> File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/pipecat/services/anthropic.py", line 178, in _process_context
    response = await api_call(**params)
                     │          └ {'tools': [], 'system': NOT_GIVEN, 'messages': [{'role': 'user', 'content': 'Ask the user if they are still there and try to ...
                     └ <bound method AsyncMessages.create of <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py", line 2051, in create
    return await self._post(
                 │    └ <bound method AsyncAPIClient.post of <anthropic.AsyncAnthropic object at 0x115915010>>
                 └ <anthropic.resources.messages.messages.AsyncMessages object at 0x1159178c0>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1842, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
                 │    │       │        │            │                  └ anthropic.AsyncStream[typing.Annotated[typing.Union[anthropic.types.raw_message_start_event.RawMessageStartEvent, anthropic.t...
                 │    │       │        │            └ True
                 │    │       │        └ FinalRequestOptions(method='post', url='/v1/messages', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=600, file...
                 │    │       └ <class 'anthropic.types.message.Message'>
                 │    └ <function AsyncAPIClient.request at 0x11567c2c0>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1536, in request
    return await self._request(
                 │    └ <function AsyncAPIClient._request at 0x11567c360>
                 └ <anthropic.AsyncAnthropic object at 0x115915010>
  File "/Users/benjamingardiner/talentora-interviewbot/venv/lib/python3.13/site-packages/anthropic/_base_client.py", line 1637, in _request
    raise self._make_status_error_from_response(err.response) from None
          │    └ <function BaseClient._make_status_error_from_response at 0x115679580>
          └ <anthropic.AsyncAnthropic object at 0x115915010>

anthropic.AuthenticationError: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}
2025-01-17 19:29:34 | DEBUG    | app.core.config:validate_required_settings:31 - Validating required settings
2025-01-17 19:29:34 | INFO     | app.core.config:validate_required_settings:47 - All required settings validated successfully
2025-01-17 19:29:34 | INFO     | __main__:<module>:58 - Starting manual bot test...
2025-01-17 19:29:34 | INFO     | __main__:test_interview_bot:29 - Creating Daily room...
2025-01-17 19:29:34 | DEBUG    | app.services.daily:create_room:25 - Creating Daily room
2025-01-17 19:29:35 | INFO     | app.services.daily:create_room:51 - Created room: https://getroborecruiter.daily.co/K37LBdWETMtjefhlFeZt
2025-01-17 19:29:35 | INFO     | __main__:test_interview_bot:34 - Room created: https://getroborecruiter.daily.co/K37LBdWETMtjefhlFeZt
2025-01-17 19:29:35 | INFO     | __main__:test_interview_bot:37 - Initializing bot...
2025-01-17 19:29:35 | INFO     | app.bot.interview_bot:__init__:43 - Initializing Interview Bot
2025-01-17 19:29:35 | DEBUG    | app.bot.interview_bot:__init__:44 - Bot configuration: {'voice_id': '79a125e8-cd45-4c13-8a67-188112f4dd22', 'max_duration': 300, 'interview_config': {'bot_name': 'Sarah', 'company_name': 'TechCorp', 'job_title': 'Senior Software Engineer', 'job_description': 'We are looking for a Senior Software Engineer with strong experience in AI/ML...', 'company_context': 'TechCorp is a leading technology company...', 'interview_questions': ['Can you tell me about your most challenging project?', 'How do you approach problem-solving?', 'What interests you about this role?']}}
2025-01-17 19:29:35 | INFO     | __main__:test_interview_bot:41 - Starting bot...
2025-01-17 19:29:35 | INFO     | app.bot.interview_bot:start:123 - Starting interview session in room: https://getroborecruiter.daily.co/K37LBdWETMtjefhlFeZt
2025-01-17 19:29:35 | INFO     | pipecat.audio.vad.vad_analyzer:set_params:69 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-01-17 19:29:35 | DEBUG    | pipecat.audio.vad.silero:__init__:113 - Loading Silero VAD model...
2025-01-17 19:29:35 | DEBUG    | pipecat.audio.vad.silero:__init__:135 - Loaded Silero VAD
2025-01-17 19:29:35 | DEBUG    | app.bot.interview_bot:start:134 - Event handlers registered
2025-01-17 19:29:35 | DEBUG    | app.bot.services.tts_service:init_tts_service:8 - Initializing Cartesia TTS
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking PipelineSource#0 -> DailyInputTransport#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyInputTransport#0 -> DeepgramSTTService#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DeepgramSTTService#0 -> IdleFrameProcessor#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking IdleFrameProcessor#0 -> LLMUserResponseAggregator#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMUserResponseAggregator#0 -> AnthropicLLMService#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking AnthropicLLMService#0 -> CartesiaTTSService#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking CartesiaTTSService#0 -> DailyOutputTransport#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking DailyOutputTransport#0 -> LLMAssistantResponseAggregator#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking LLMAssistantResponseAggregator#0 -> PipelineSink#0
2025-01-17 19:29:35 | DEBUG    | app.bot.interview_bot:_setup_pipeline:75 - Pipeline setup complete
2025-01-17 19:29:35 | DEBUG    | app.bot.interview_bot:start:138 - Daily transport is now active
2025-01-17 19:29:35 | DEBUG    | app.bot.interview_bot:_run_pipeline:84 - Starting pipeline runner
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Source#0 -> Pipeline#0
2025-01-17 19:29:35 | DEBUG    | pipecat.processors.frame_processor:link:150 - Linking Pipeline#0 -> Sink#0
2025-01-17 19:29:35 | DEBUG    | pipecat.pipeline.runner:run:27 - Runner PipelineRunner#0 started running PipelineTask#0
2025-01-17 19:29:35 | DEBUG    | pipecat.services.deepgram:_connect:202 - Connecting to Deepgram
2025-01-17 19:29:35 | DEBUG    | pipecat.services.cartesia:_connect_websocket:202 - Connecting to Cartesia
2025-01-17 19:29:35 | INFO     | pipecat.transports.services.daily:join:322 - Joining https://getroborecruiter.daily.co/K37LBdWETMtjefhlFeZt
2025-01-17 19:29:36 | INFO     | pipecat.transports.services.daily:join:340 - Joined https://getroborecruiter.daily.co/K37LBdWETMtjefhlFeZt
2025-01-17 19:29:36 | INFO     | pipecat.transports.services.daily:_start_transcription:360 - Enabling transcription with settings language='en' tier=None model='nova-2-general' profanity_filter=True redact=False endpointing=True punctuate=True includeRawResponse=True extra={'interim_results': True}
2025-01-17 19:29:37 | DEBUG    | pipecat.transports.services.daily:on_transcription_started:638 - Transcription started: {'instanceId': 'a1f2f6b7-b1ac-4202-85e5-d446cb6c3d3f', 'language': 'en', 'model': 'nova-2-general', 'startedBy': '75be5eef-c2d2-4845-a36f-a5ef13171483', 'transcriptId': '537f7b84-eb90-4b07-b412-607868527b4a'}
2025-01-17 19:29:37 | INFO     | app.bot.handlers.event_handlers:on_joined:8 - Bot joined room with data: {'callConfig': {'initialMicrophoneEnabled': True, 'terseLoggingEnabled': False, 'permissionsOnJoin': {'canSend': ['camera', 'microphone', 'screenVideo', 'screenAudio', 'customVideo', 'customAudio'], 'hasPresence': True, 'canAdmin': ['participants', 'streaming', 'transcription']}, 'recordingMode': 'off', 'roomExpiration': 1737161974, 'ejectAtRoomExpiration': True, 'roomName': 'K37LBdWETMtjefhlFeZt', 'defaultStreamingEndpoints': [], 'ejectAtTokenExpiration': False, 'isOwner': True, 'tokenExpiration': 1737163774, 'initialCameraEnabled': False}, 'meetingSession': {'id': '40115290-cea0-4e83-a56b-5488e2949b65'}, 'participants': {'local': {'id': '75be5eef-c2d2-4845-a36f-a5ef13171483', 'info': {'isOwner': True, 'isLocal': True, 'permissions': {'canAdmin': ['transcription', 'participants', 'streaming'], 'canSend': ['microphone', 'screenAudio', 'customVideo', 'customAudio', 'camera', 'screenVideo'], 'hasPresence': True}, 'userName': 'Sarah', 'joinedAt': 1737160175}, 'media': {'screenVideo': {'subscribed': 'unsubscribed', 'offReasons': ['user'], 'state': 'off'}, 'camera': {'state': 'off', 'subscribed': 'unsubscribed', 'offReasons': ['user']}, 'microphone': {'subscribed': 'unsubscribed', 'track': {'id': 'bb57d844-2453-414a-974d-43263dcd202f'}, 'state': 'playable'}, 'customAudio': {}, 'customVideo': {}, 'screenAudio': {'subscribed': 'unsubscribed', 'state': 'off', 'offReasons': ['user']}}}}}
2025-01-17 19:29:42 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:29:42 | DEBUG    | pipecat.services.anthropic:_process_context:241 - Cache creation input tokens: 0
2025-01-17 19:29:42 | DEBUG    | pipecat.services.anthropic:_process_context:244 - Cache read input tokens: 0
2025-01-17 19:29:42 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [Sure, I can help with that.]
2025-01-17 19:29:42 | DEBUG    | pipecat.transports.base_output:_bot_started_speaking:203 - Bot started speaking
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ Here's a message to check if the user is still there and prompt for input:]
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ Are you still there?]
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ If so, I'd love to hear from you!]
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ What would you like to talk about?]
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ You can ask me a question, share something interesting, or even just say hello.]
2025-01-17 19:29:43 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ I'm here and ready to chat about any topic you're interested in.]
2025-01-17 19:29:59 | DEBUG    | pipecat.transports.base_output:_bot_stopped_speaking:210 - Bot stopped speaking
2025-01-17 19:30:04 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:30:04 | INFO     | pipecat.transports.services.daily:on_participant_joined:620 - Participant joined 7118705e-3003-4ee9-8a22-d1e526c2184b
2025-01-17 19:30:04 | INFO     | app.bot.handlers.event_handlers:on_participant_joined:16 - Participant joined: {'info': {'joinedAt': 1737160204, 'permissions': {'canSend': ['screenAudio', 'screenVideo', 'camera', 'customAudio', 'customVideo', 'microphone'], 'hasPresence': True, 'canAdmin': []}, 'isOwner': False, 'userName': 'Ben', 'isLocal': False}, 'id': '7118705e-3003-4ee9-8a22-d1e526c2184b', 'media': {'screenVideo': {'subscribed': 'unsubscribed', 'offReasons': ['user'], 'state': 'off'}, 'microphone': {'state': 'loading', 'subscribed': 'subscribed'}, 'customVideo': {}, 'camera': {'state': 'off', 'subscribed': 'unsubscribed', 'offReasons': ['user']}, 'screenAudio': {'subscribed': 'subscribed', 'state': 'off', 'offReasons': ['user']}, 'customAudio': {}}}
2025-01-17 19:30:04 | DEBUG    | pipecat.services.anthropic:_process_context:241 - Cache creation input tokens: 0
2025-01-17 19:30:04 | DEBUG    | pipecat.services.anthropic:_process_context:244 - Cache read input tokens: 0
2025-01-17 19:30:04 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [Sure, I can do that.]
2025-01-17 19:30:05 | DEBUG    | pipecat.transports.base_output:_bot_started_speaking:203 - Bot started speaking
2025-01-17 19:30:05 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ Here's a message to the user:]
2025-01-17 19:30:05 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ Hello!]
2025-01-17 19:30:05 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ Are you still there?]
2025-01-17 19:30:05 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ If you are, could you please respond with anything - maybe tell me how your day is going or what's on your mind?]
2025-01-17 19:30:05 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ I'm here to chat and assist you with any questions or topics you'd like to discuss.]
2025-01-17 19:30:15 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:124 - User started speaking
2025-01-17 19:30:15 | DEBUG    | pipecat.transports.base_output:_bot_stopped_speaking:210 - Bot stopped speaking
2025-01-17 19:30:16 | DEBUG    | pipecat.transports.base_input:_handle_interruptions:131 - User stopped speaking
2025-01-17 19:30:16 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Hello?"}]
2025-01-17 19:30:16 | DEBUG    | pipecat.services.anthropic:_process_context:241 - Cache creation input tokens: 0
2025-01-17 19:30:16 | DEBUG    | pipecat.services.anthropic:_process_context:244 - Cache read input tokens: 0
2025-01-17 19:30:16 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [Hello!]
2025-01-17 19:30:16 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [ How can I assist you today?]
2025-01-17 19:30:16 | DEBUG    | pipecat.transports.base_output:_bot_started_speaking:203 - Bot started speaking
2025-01-17 19:30:19 | DEBUG    | pipecat.transports.base_output:_bot_stopped_speaking:210 - Bot stopped speaking
2025-01-17 19:30:20 | INFO     | pipecat.transports.services.daily:on_participant_left:630 - Participant left 7118705e-3003-4ee9-8a22-d1e526c2184b
2025-01-17 19:30:20 | INFO     | app.bot.handlers.event_handlers:on_participant_left:36 - Participant 7118705e-3003-4ee9-8a22-d1e526c2184b left: leftCall
2025-01-17 19:30:24 | DEBUG    | pipecat.services.anthropic:_process_context:150 - Generating chat: NOT_GIVEN | [{"role": "user", "content": "Ask the user if they are still there and try to prompt for some input."}]
2025-01-17 19:30:25 | DEBUG    | pipecat.services.anthropic:_process_context:241 - Cache creation input tokens: 0
2025-01-17 19:30:25 | DEBUG    | pipecat.services.anthropic:_process_context:244 - Cache read input tokens: 0
2025-01-17 19:30:25 | DEBUG    | pipecat.services.cartesia:run_tts:316 - Generating TTS: [Hello!]
